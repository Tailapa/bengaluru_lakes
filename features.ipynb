{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b770a2",
   "metadata": {},
   "source": [
    "* **occurrence (JRC)**: A value from `0–100`. If it's 100, the lake has had water consistently for decades. If it's low, it's a seasonal or disappearing lake.\n",
    "\n",
    "* **VV (Sentinel-1)**: Radar backscatter. Values around -20 or lower usually indicate standing water/inundation. Higher values indicate buildings or rough ground.\n",
    "\n",
    "* **elevation (SRTM)**: The height above sea level. Useful for identifying if a lake is at the bottom of a catchment (high flood risk) or perched higher up.\n",
    "\n",
    "* **precipitation (GPM)**: The total accumulated rainfall \"signal\" for the 5-year period. You can divide this by 5 to get the average annual rainfall for that specific lake's micro-climate.\n",
    "\n",
    "    * `Map` (ESA WorldCover): A class code.\n",
    "\n",
    "    * `80`: Water bodies\n",
    "\n",
    "    * `50`: Built-up (Buildings)\n",
    "\n",
    "    * `10`: Tree cover\n",
    "\n",
    "    * `40`: Cropland\n",
    "\n",
    "**For Encroachment Study**:\n",
    "* Once I have this CSV, I can correlate `in_build_ha` with the Map value from ESA.\n",
    "\n",
    "* If `in_build_ha` is high and the ESA class is 50 (Built-up), two independent satellite datasets are confirming the encroachment.\n",
    "\n",
    "* If `in_build_ha` is high but the occurrence (JRC) is also high, it means buildings are being constructed in areas that are historically underwater.\n",
    "\n",
    "#### SRTM 30m DEM\n",
    "\n",
    "* **Elevation**: This is the raw value provided by the **SRTM band**.\n",
    "\n",
    "* **Slope**: Calculated by comparing the elevation of a pixel to its neighbors.\n",
    "\n",
    "* **Flow Direction & Accumulation**: These require \"Pit Filling\" (removing tiny errors in the DEM). While you can calculate them, it is computationally heavy. Most researchers use **WWF/HydroSHEDS**, which was built directly from SRTM data.\n",
    "\n",
    "* **Local Depressions**: These are \"Sinks\" where water would naturally pool. These are critical for lake analysis as they define the natural basin.\n",
    "\n",
    "#### What these results will tell you:\n",
    "* **Slope**: If a lake has a very low slope (flat) but high in_build_ha, it means buildings are being built on a natural floodplain, making the area extremely prone to waterlogging.\n",
    "\n",
    "* **Flow Accumulation**: A high value here indicates that the lake is a major \"drainage node.\" If this lake is 80% encroached, the water that used to accumulate here will now be pushed into the surrounding streets.\n",
    "\n",
    "* **Sink Depth (sink_depth)**: If this value is positive, it identifies a \"bowl\" in the landscape. If buildings are inside a zone where sink_depth > 0, they are physically sitting in a location where water is geographically \"supposed\" to be.\n",
    "\n",
    "* **Flow Direction**: Tells you exactly which way the sewage or overflow will move when the lake reaches capacity.\n",
    "\n",
    "#### Summary of Source IDs:\n",
    "* **Elevation/Slope**: USGS/SRTMGL1_003\n",
    "\n",
    "* **Flow/Basins**: WWF/HydroSHEDS/15ACC (Accumulation) and WWF/HydroSHEDS/15VFD (Direction).\n",
    "\n",
    "* **Precise Hydrology**: MERIT/Hydro/v1_0_1 (This is a newer, \"cleaned\" version of SRTM that is better for flat urban areas like Bengaluru)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1bc7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Initialize\n",
    "PROJECT_ID = 'bengaluru-lakes-485612' \n",
    "ee.Initialize(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577885e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Lakes\n",
    "df = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "lake_points = ee.FeatureCollection([\n",
    "    ee.Feature(ee.Geometry.Point([row['lon'], row['lat']]), {'name': row['name']})\n",
    "    for i, row in df.iterrows()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Topographic Datasets\n",
    "# A. Raw Elevation\n",
    "srtm = ee.Image(\"USGS/SRTMGL1_003\")\n",
    "elevation = srtm.select('elevation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1d0f6",
   "metadata": {},
   "source": [
    "### Visualising Bengaluru’s Topographic Context for Lake Analysis\n",
    "\n",
    "In this code block, we are setting up a **topographic and spatial context** to understand Bengaluru’s lakes not as isolated points, but as features embedded within a **sloping urban landscape**.\n",
    "\n",
    "First, we define the **administrative boundary of Bengaluru Urban district** using the FAO GAUL Level-2 dataset. By filtering for `ADM2_NAME = Bangalore Urban`, we extract a precise polygon that represents the city’s official extent. This boundary becomes the spatial frame for all subsequent analysis, ensuring that global datasets are restricted strictly to Bengaluru.\n",
    "\n",
    "Next, we load **SRTM elevation data**, which provides a near-surface digital elevation model at ~30 m resolution. Instead of working with the global DEM, we **clip** it to the Bengaluru boundary. This step is crucial: it removes irrelevant terrain outside the city and ensures that elevation and slope values are interpreted only within the urban hydrological system we care about.\n",
    "\n",
    "Using the clipped elevation, we then compute **slope** with `ee.Terrain.slope`. Slope is derived from elevation gradients and represents how steep or flat the ground is at each location. Clipping the slope layer again to the Bengaluru boundary keeps the analysis spatially consistent. This slope layer is especially important for lake studies, because it helps reveal whether lakes sit in natural “bowls” (depressions) or on unnaturally flattened surfaces.\n",
    "\n",
    "We then initialise an interactive **geemap map**, centring it on Bengaluru at a city-scale zoom level. This makes the map immediately interpretable as an urban system rather than a regional or national view.\n",
    "\n",
    "For visual interpretation, you define colour palettes:\n",
    "- The **elevation palette** highlights Bengaluru’s plateau structure (roughly 800–1000 m), allowing you to see ridges, valleys, and relative height differences across the city.\n",
    "- The **slope palette** simplifies interpretation by mapping flat areas to white and steeper terrain to black, making potential lake basins and drainage gradients visually obvious.\n",
    "\n",
    "Finally, you layer everything together:\n",
    "- The **Bengaluru boundary** is drawn as a red outline to anchor spatial reference.\n",
    "- The **elevation layer** shows the city’s vertical structure.\n",
    "- The **slope layer** reveals geomorphic features such as depressions and embankments.\n",
    "- Your **lake points from the CSV** are overlaid in yellow, allowing you to visually assess where each lake sits relative to elevation and slope.\n",
    "\n",
    "Conceptually, this step lets you answer questions like:\n",
    "- Are lakes located in low-lying terrain or on ridges?\n",
    "- Do flood-prone lakes sit in flat, filled-in areas?\n",
    "- How does the city’s natural slope influence water movement toward or away from lakes?\n",
    "\n",
    "In short, you are building a **geomorphological backdrop** that grounds all later flood, storage, and encroachment analyses in Bengaluru’s real physical landscape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87652dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Get the Bengaluru Boundary\n",
    "# We use the GAUL Level 2 dataset and filter for 'Bangalore Urban'\n",
    "bengaluru_boundary = ee.FeatureCollection(\"FAO/GAUL/2015/level2\") \\\n",
    "    .filter(ee.Filter.eq('ADM2_NAME', 'Bangalore Urban'))\n",
    "\n",
    "# 3. Load and Clip the Topographic Data\n",
    "srtm = ee.Image(\"USGS/SRTMGL1_003\")\n",
    "# Use .clip() to cut the global image to the Bengaluru shape\n",
    "elevation_clipped = srtm.select('elevation').clip(bengaluru_boundary)\n",
    "slope_clipped = ee.Terrain.slope(elevation_clipped).clip(bengaluru_boundary)\n",
    "\n",
    "# 4. Setup the Map\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(bengaluru_boundary, 11) # Zoom specifically to the city\n",
    "\n",
    "# --- Visualization Parameters ---\n",
    "elev_viz = {\n",
    "    'min': 800, \n",
    "    'max': 1000, \n",
    "    'palette': ['#313695', '#4575b4', '#abd9e9', '#fee090', '#f46d43', '#d73027']\n",
    "}\n",
    "\n",
    "slope_viz = {\n",
    "    'min': 0, \n",
    "    'max': 5, \n",
    "    'palette': ['white', 'black'] # White = Flat, Black = Steep\n",
    "}\n",
    "\n",
    "# --- Add Layers to Map ---\n",
    "# Add the Boundary (just the outline)\n",
    "Map.addLayer(bengaluru_boundary.style(fillColor='00000000', color='red', width=2), {}, 'Bengaluru Boundary')\n",
    "\n",
    "# Add Clipped Elevation\n",
    "Map.addLayer(elevation_clipped, elev_viz, 'Bengaluru Elevation (m)')\n",
    "\n",
    "# Add Clipped Slope (helps see the lake \"bowls\")\n",
    "Map.addLayer(slope_clipped, slope_viz, 'Bengaluru Slope (Degrees)')\n",
    "\n",
    "# Add your Lake Points from the CSV\n",
    "# (Assuming lake_points variable is already defined from previous step)\n",
    "Map.addLayer(lake_points, {'color': 'yellow'}, 'Lake Points (from CSV)')\n",
    "\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb08b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the topography data\n",
    "df_topo = pd.read_csv('lake_flatness_analysis.csv')\n",
    "\n",
    "# Statistics for explanation\n",
    "slope_stats = df_topo['slope'].describe()\n",
    "elev_stats = df_topo['elevation'].describe()\n",
    "\n",
    "# Get extremes for examples\n",
    "flattest = df_topo.nsmallest(3, 'slope')\n",
    "steepest = df_topo.nlargest(3, 'slope')\n",
    "highest = df_topo.nlargest(3, 'elevation')\n",
    "lowest = df_topo.nsmallest(3, 'elevation')\n",
    "\n",
    "# Print summary for my use\n",
    "print(\"Slope Stats:\\n\", slope_stats)\n",
    "print(\"\\nElevation Stats:\\n\", elev_stats)\n",
    "\n",
    "# Visualizing the distribution of Slope\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df_topo['slope'], bins=20, color='#2ca02c', alpha=0.7, edgecolor='black')\n",
    "plt.axvline(df_topo['slope'].median(), color='red', linestyle='dashed', linewidth=1, label=f'Median: {df_topo[\"slope\"].median():.2f}°')\n",
    "plt.title('Distribution of Mean Slope across Bengaluru Lakes')\n",
    "plt.xlabel('Mean Slope (Degrees)')\n",
    "plt.ylabel('Number of Lakes')\n",
    "plt.legend()\n",
    "plt.savefig('slope_distribution.png')\n",
    "\n",
    "# Visualizing Elevation (The Cascade)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df_topo['elevation'], bins=20, color='#1f77b4', alpha=0.7, edgecolor='black')\n",
    "plt.axvline(df_topo['elevation'].median(), color='red', linestyle='dashed', linewidth=1, label=f'Median: {df_topo[\"elevation\"].median():.2f}m')\n",
    "plt.title('Elevation Distribution (The Bengaluru Plateau)')\n",
    "plt.xlabel('Elevation (Meters above Sea Level)')\n",
    "plt.ylabel('Number of Lakes')\n",
    "plt.legend()\n",
    "plt.savefig('elevation_distribution.png')\n",
    "\n",
    "# Data for the final explanation\n",
    "print(\"\\nFlattest Lakes (The 'Tables'):\")\n",
    "print(flattest[['name', 'slope']])\n",
    "print(\"\\nSteepest Lakes (The 'Bowls'):\")\n",
    "print(steepest[['name', 'slope']])\n",
    "print(\"\\nHighest Lakes (Upstream):\")\n",
    "print(highest[['name', 'elevation']])\n",
    "print(\"\\nLowest Lakes (Downstream):\")\n",
    "print(lowest[['name', 'elevation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c906eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the flatness analysis data\n",
    "df_topo = pd.read_csv('data/lake_flatness_analysis.csv')\n",
    "\n",
    "# Statistics for Slope\n",
    "slope_mean = df_topo['slope'].mean()\n",
    "slope_min = df_topo['slope'].min()\n",
    "slope_max = df_topo['slope'].max()\n",
    "\n",
    "# Create categories\n",
    "def categorize_slope(s):\n",
    "    if s < 1.5: return 'Flat (Table/Modified)'\n",
    "    elif s < 3.0: return 'Gentle (Basin)'\n",
    "    else: return 'Steep (Valley/Bund)'\n",
    "\n",
    "df_topo['category'] = df_topo['slope'].apply(categorize_slope)\n",
    "\n",
    "# Get examples for each\n",
    "examples = df_topo.sort_values('slope').groupby('category').head(3)\n",
    "\n",
    "print(\"Slope Statistics:\")\n",
    "print(f\"Mean: {slope_mean:.2f}, Min: {slope_min:.2f}, Max: {slope_max:.2f}\")\n",
    "print(\"\\nRepresentative Examples:\")\n",
    "print(examples[['name', 'slope', 'elevation', 'category']])\n",
    "\n",
    "# Plot Elevation vs Slope to see if there is a pattern\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_topo['elevation'], df_topo['slope'], alpha=0.5, c='teal')\n",
    "plt.axhline(y=1.5, color='r', linestyle='--', label='Flatness Threshold')\n",
    "plt.xlabel('Elevation (m)')\n",
    "plt.ylabel('Mean Slope (Degrees)')\n",
    "plt.title('Bengaluru Lakes: Slope vs Elevation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('slope_elevation_viz.png')\n",
    "\n",
    "# Count by category\n",
    "category_counts = df_topo['category'].value_counts()\n",
    "print(\"\\nCategory Distribution:\")\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b7066",
   "metadata": {},
   "source": [
    "## Understanding Slope and Elevation in Bengaluru’s Lake System\n",
    "\n",
    "To understand **slope** and **elevation** in the context of Bengaluru’s lakes, it is important to move beyond seeing lakes as simple *points on a map*.  \n",
    "They must instead be understood as **physical vessels embedded in a sloping landscape** that control how water is stored and released.\n",
    "\n",
    "The following interpretation explains how to read the outputs from `lake_flatness_analysis.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Slope: The “Bowl vs. Table” Logic\n",
    "\n",
    "In geomorphology, a lake naturally behaves like a **bowl**.  \n",
    "Its sides should slope inward, guiding water toward a central depression.  \n",
    "The **Mean Slope** within a lake boundary therefore represents its **geomorphic integrity**.\n",
    "\n",
    "---\n",
    "\n",
    "### A. The “Table” (Slope < 1.5°)\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- Lakes in this category (e.g., *B. Narayanapura – 0.74°*) are **unnaturally flat**.\n",
    "\n",
    "**What this implies**\n",
    "\n",
    "- Such low slope indicates that the natural bowl has been **filled or leveled**.\n",
    "- This typically results from:\n",
    "  - Construction activity  \n",
    "  - Debris dumping  \n",
    "  - Sedimentation and encroachment\n",
    "\n",
    "**Hydrological consequence**\n",
    "\n",
    "- A “table-like” lake cannot **store** water.\n",
    "- Instead of holding runoff, it allows water to **spread outward**, flooding nearby streets and layouts.\n",
    "\n",
    "---\n",
    "\n",
    "### B. The “Bowl” (Slope 1.5° – 3.0°)\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- This range represents the **optimal or “Goldilocks” zone** for Bengaluru’s plateau terrain.\n",
    "\n",
    "**What this implies**\n",
    "\n",
    "- Lakes in this category (e.g., *Vidyaranyapura Kere*) retain a **functional depression**.\n",
    "- The slope is sufficient to:\n",
    "  - Channel runoff inward  \n",
    "  - Maintain internal drainage toward the lake center\n",
    "\n",
    "**Hydrological consequence**\n",
    "\n",
    "- These lakes can still perform their role as **urban buffers**, absorbing rainfall and reducing downstream flooding.\n",
    "\n",
    "---\n",
    "\n",
    "### C. The “Vessel” (Slope > 3.0°)\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- These are lakes with **steep sides**, often natural valleys or lakes reinforced with high bunds (embankments).\n",
    "\n",
    "**What this implies**\n",
    "\n",
    "- Lakes such as *Nayandahalli (3.03°)* are often protected by their own steepness.\n",
    "- Steep terrain:\n",
    "  - Discourages encroachment  \n",
    "  - Raises construction costs  \n",
    "  - Acts as a **natural defense mechanism**\n",
    "\n",
    "**Hydrological consequence**\n",
    "\n",
    "- Such lakes retain storage capacity and are less likely to be flattened or illegally occupied.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Elevation: The “Hydrological Rank”\n",
    "\n",
    "Bengaluru historically functioned as a **cascade lake system**, where lakes were linked in a downstream chain.  \n",
    "**Elevation** determines a lake’s position and role within this hydrological hierarchy.\n",
    "\n",
    "---\n",
    "\n",
    "### A. Headwater Lakes (High Elevation > 900 m)\n",
    "\n",
    "**Examples**\n",
    "\n",
    "- Vidyaranyapura (911 m)  \n",
    "- Gantiganahalli (901 m)\n",
    "\n",
    "**Role**\n",
    "\n",
    "- These are the **first receivers of rainfall**.\n",
    "- They regulate flows to all downstream lakes.\n",
    "\n",
    "**Risk implication**\n",
    "\n",
    "- If headwater lakes are:\n",
    "  - Encroached  \n",
    "  - Flattened  \n",
    "  - Hydrologically disconnected  \n",
    "\n",
    "  then the **entire cascade below them fails**, amplifying flood risk citywide.\n",
    "\n",
    "---\n",
    "\n",
    "### B. Terminal Lakes (Low Elevation < 850 m)\n",
    "\n",
    "**Examples**\n",
    "\n",
    "- Hosakerahalli (833 m)  \n",
    "- Nayandahalli (810 m)\n",
    "\n",
    "**Role**\n",
    "\n",
    "- These function as **final sinks** in the system.\n",
    "- They receive:\n",
    "  - Overflow from upstream lakes  \n",
    "  - Urban runoff  \n",
    "  - Sewage and stormwater\n",
    "\n",
    "**Risk implication**\n",
    "\n",
    "- Because they sit at the bottom of the city’s drainage gradient, they are **structurally flood-prone**, even if well maintained.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Visualising the Conflict: Slope vs Elevation\n",
    "\n",
    "The slope–elevation plot derived from the dataset reveals a clear pattern:\n",
    "\n",
    "- Most Bengaluru lakes cluster between **870 m and 910 m**, corresponding to the Deccan plateau surface.\n",
    "\n",
    "---\n",
    "\n",
    "### The “Danger Zone”\n",
    "\n",
    "**Definition**\n",
    "\n",
    "- Lakes with:\n",
    "  - **High elevation (> 900 m)**  \n",
    "  - **Low slope (< 1.5°)**\n",
    "\n",
    "**Why this is critical**\n",
    "\n",
    "- These lakes were historically **key sponges at the top of the watershed**.\n",
    "- Urban development has flattened them into near-level land.\n",
    "\n",
    "**Outcome**\n",
    "\n",
    "- During heavy rainfall:\n",
    "  - Water cannot be stored locally  \n",
    "  - Runoff accelerates downslope  \n",
    "  - Downstream areas experience **sudden and intense flooding**\n",
    "\n",
    "**Real-world relevance**\n",
    "\n",
    "- This mechanism explains recurring floods in downstream zones such as:\n",
    "  - Silk Board  \n",
    "  - Bellandur  \n",
    "  - Outer Ring Road corridor\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "Slope determines a lake’s **capacity to store water**, while elevation determines its **position in the urban drainage hierarchy**.  \n",
    "Flood risk in Bengaluru emerges most sharply where **high-elevation lakes lose their natural bowl-shaped geometry**.\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a45fb6",
   "metadata": {},
   "source": [
    "# Explanation of the Lake Flatness and Topography Analysis Code\n",
    "\n",
    "This script links **lake attribute data** with **terrain information** from satellite-derived elevation models to understand how lake geometry and surrounding topography influence flood behaviour in Bengaluru.\n",
    "\n",
    "---\n",
    "\n",
    "### Creating Lake Polygons from Area Values\n",
    "`features = []`\\\n",
    "`for i, row in df.iterrows():`\\\n",
    "    `radius = np.sqrt((row['potential_ha'] * 10000) / np.pi)`\\\n",
    "    `geom = ee.Geometry.Point([row['lon'], row['lat']]).buffer(radius)`\\\n",
    "    `features.append(ee.Feature(geom, {'name': row['name'], 'id': i}))`\n",
    "    \n",
    "**What is happening conceptually**\n",
    "\n",
    "* The CSV only contains points, not lake boundaries.\n",
    "\n",
    "* To approximate lake extents:\n",
    "\n",
    "    * Each lake is modelled as a circle.\n",
    "\n",
    "    * The radius is calculated so that the circle’s area equals the lake’s potential_ha.\n",
    "\n",
    "**Key logic**\n",
    "\n",
    "* 1 hectare = 10,000 m²\n",
    "\n",
    "* Area of circle = πr²\n",
    "\n",
    "* Radius = √(Area / π)\n",
    "\n",
    "**Why this is done**\n",
    "\n",
    "* Enables terrain statistics to be calculated over a lake footprint, not just a point.\n",
    "\n",
    "* Creates a consistent and reproducible approximation of lake geometry.\n",
    "\n",
    "---\n",
    "\n",
    "### Creating an Earth Engine FeatureCollection\n",
    "`lake_polygons = ee.FeatureCollection(features)`\n",
    "\n",
    "\n",
    "**What is happening**\n",
    "\n",
    "* All individual lake polygons are bundled into a single FeatureCollection.\n",
    "\n",
    "* This collection can now be used for bulk spatial operations in Earth Engine.\n",
    "\n",
    "---\n",
    "\n",
    "### Preparing Topographic Layers (Elevation and Slope)\n",
    "`srtm = ee.Image(\"USGS/SRTMGL1_003\")`\\\n",
    "`elevation = srtm.select('elevation')`\\\n",
    "`slope = ee.Terrain.slope(elevation).rename('slope')`\n",
    "\n",
    "\n",
    "**What is happening**\n",
    "\n",
    "* Loads the **SRTM 30 m Digital Elevation Model**.\n",
    "\n",
    "* Extracts:\n",
    "\n",
    "    * **elevation** → absolute height above sea level (meters).\n",
    "\n",
    "    * **slope** → steepness derived from elevation (degrees).\n",
    "\n",
    "**Why slope matters**\n",
    "\n",
    "* Indicates whether a lake basin is:\n",
    "\n",
    "    * Naturally bowl-shaped\n",
    "\n",
    "    * Artificially flattened\n",
    "\n",
    "    * Steep and resistant to encroachment\n",
    "\n",
    "---\n",
    "\n",
    "### Combining Elevation and Slope into a Single Stack\n",
    "`topo_stack = ee.Image.cat([elevation, slope])`\n",
    "\n",
    "\n",
    "**What is happening**\n",
    "\n",
    "* Elevation and slope are stacked into one multi-band image.\n",
    "\n",
    "* Allows both variables to be analysed simultaneously in one operation.\n",
    "\n",
    "---\n",
    "\n",
    "### Extracting Mean Terrain Values for Each Lake\n",
    "`stats = topo_stack.reduceRegions(`\\\n",
    "    `collection=lake_polygons,`\\\n",
    "    `reducer=ee.Reducer.mean(),`\\\n",
    "    `scale=30`\\\n",
    "`)`\n",
    "\n",
    "\n",
    "**What is happening**\n",
    "\n",
    "* For each lake polygon:\n",
    "\n",
    "    * Earth Engine calculates the mean elevation.\n",
    "\n",
    "    * Earth Engine calculates the mean slope.   \n",
    "\n",
    "    * The scale=30 ensures calculations match SRTM’s spatial resolution.\n",
    "\n",
    "**Output**\n",
    "\n",
    "* A FeatureCollection where each lake now has:\n",
    "\n",
    "    * mean_elevation\n",
    "\n",
    "    * mean_slope\n",
    "\n",
    "---\n",
    "\n",
    "### Converting Results to a DataFrame\n",
    "`df_results = geemap.ee_to_df(stats)`\n",
    "\n",
    "\n",
    "**What is happening**\n",
    "\n",
    "* Transfers results from Earth Engine (server-side) to Python (client-side).\n",
    "\n",
    "* Converts spatial results into a Pandas DataFrame for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbbf37",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Slope and Elevation\n",
    "This script performs a **lake-specific topographic analysis** using **exact lake boundary geometries** (not circular proxies). It converts lake polygons stored in a CSV into **Earth Engine FeatureCollections**, then extracts **mean elevation and mean slope** within each lake’s true spatial footprint.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Preparing Lake Attributes and Boundaries\n",
    "- Two CSV files are loaded:\n",
    "  - One containing **lake attributes** (names, metadata).\n",
    "  - Another containing **exact lake boundary geometries** stored as **WKT (Well-Known Text)**.\n",
    "- Duplicate lake names in the boundary file are removed to ensure a **one-to-one join**.\n",
    "- The attribute table and boundary table are **merged on lake name**, attaching polygon geometry to each lake record.\n",
    "\n",
    "**Key idea:** this step upgrades the analysis from *approximate circular buffers* to **true lake outlines**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Converting WKT Geometries into Earth Engine Features\n",
    "- Each lake’s geometry is read from the WKT string using **Shapely**.\n",
    "- Two cases are handled explicitly:\n",
    "  - **Polygon** → a single contiguous lake boundary.\n",
    "  - **MultiPolygon** → lakes with multiple disconnected basins or islands.\n",
    "- Coordinates are extracted from Shapely objects and converted into:\n",
    "  - **ee.Geometry.Polygon** or\n",
    "  - **ee.Geometry.MultiPolygon**\n",
    "- Each geometry is wrapped as an **ee.Feature** with the lake name as metadata.\n",
    "- All features are combined into a single **ee.FeatureCollection**.\n",
    "\n",
    "**Key idea:** Earth Engine cannot read WKT directly, so this step bridges **local vector geometry** → **cloud-based geospatial analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Preparing Topographic Layers\n",
    "- **SRTM elevation data** (~30 m resolution) is loaded.\n",
    "- Two topographic variables are derived:\n",
    "  - **Elevation** → absolute height of the lake basin.\n",
    "  - **Slope** → steepness of terrain inside the lake footprint.\n",
    "- These layers are stacked into a single **multi-band image** for efficient processing.\n",
    "\n",
    "**Key idea:** elevation gives **hydrological position**, slope gives **geomorphic integrity**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Extracting Mean Topography within Exact Lake Boundaries\n",
    "- `reduceRegions` is used with the **lake polygon FeatureCollection**.\n",
    "- For each lake polygon:\n",
    "  - Mean **elevation** is computed.\n",
    "  - Mean **slope** is computed.\n",
    "- Extraction is done at **30 m scale**, matching the SRTM resolution.\n",
    "\n",
    "**Key idea:** statistics are computed **only inside the real lake boundaries**, not across buffers or surrounding land.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5: Exporting Results\n",
    "- The Earth Engine results are converted into a **Pandas DataFrame**.\n",
    "- The final table is saved as `lake_slope_elevation.csv`.\n",
    "\n",
    "Each row in the output represents:\n",
    "- one lake\n",
    "- its **mean elevation**\n",
    "- its **mean slope**\n",
    "\n",
    "---\n",
    "\n",
    "#### Conceptual Significance\n",
    "This workflow measures **how flat or bowl-shaped each lake actually is**, using its **true spatial extent**. Flat, low-slope lakes are more likely to be **filled, encroached, or hydrologically compromised**, while steeper basins indicate **better-preserved lake morphology**.\n",
    "\n",
    "---\n",
    "\n",
    "#### One-line takeaway\n",
    "We are extracting **physically meaningful topographic indicators** (slope and elevation) **directly from exact lake boundaries**, enabling robust analysis of lake degradation and flood vulnerability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50365b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "from shapely import wkt # To parse the WKT geometry\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "df = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "df_boundary = pd.read_csv('data/lake_polygon_boundaries.csv')\n",
    "df_boundary = df_boundary.drop_duplicates(subset='name')\n",
    "df = df.merge(df_boundary, on = 'name', how = 'left')\n",
    "\n",
    "# 2. Convert Pandas DataFrame (with geometries) to EE FeatureCollection\n",
    "features = []\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['geometry']):\n",
    "        # Parse the WKT string\n",
    "        poly = wkt.loads(row['geometry'])\n",
    "        \n",
    "        if isinstance(poly, Polygon):\n",
    "            # Single Polygon: Create a list containing one ring\n",
    "            coords = [list(poly.exterior.coords)]\n",
    "            geom = ee.Geometry.Polygon(coords)\n",
    "            \n",
    "        elif isinstance(poly, MultiPolygon):\n",
    "            # MultiPolygon: Iterate through all constituent polygons\n",
    "            all_rings = []\n",
    "            for p in poly.geoms:\n",
    "                all_rings.append([list(p.exterior.coords)])\n",
    "            geom = ee.Geometry.MultiPolygon(all_rings)\n",
    "            \n",
    "        features.append(ee.Feature(geom, {'name': row['name']}))\n",
    "\n",
    "lake_polygons = ee.FeatureCollection(features)\n",
    "\n",
    "# 3. Topographic Analysis\n",
    "srtm = ee.Image(\"USGS/SRTMGL1_003\")\n",
    "elevation = srtm.select('elevation')\n",
    "slope = ee.Terrain.slope(elevation).rename('slope')\n",
    "topo_stack = ee.Image.cat([elevation, slope])\n",
    "\n",
    "# 4. Extract Stats (Mean values within the EXACT boundaries)\n",
    "stats = topo_stack.reduceRegions(\n",
    "    collection=lake_polygons,\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    scale=30 \n",
    ")\n",
    "\n",
    "# 5. Export to CSV\n",
    "df_results = geemap.ee_to_df(stats)\n",
    "df_results.to_csv('data/lake_slope_elevation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecec04f",
   "metadata": {},
   "source": [
    "## Extracting Hydrological Context for Bengaluru’s Lakes\n",
    "\n",
    "This section of the code prepares the **spatial boundary**, **lake locations**, and **hydrological flow layers** needed to understand how water moves across Bengaluru and interacts with its lakes.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Defining the Bengaluru Urban Boundary\n",
    "\n",
    "```python\n",
    "bengaluru_boundary = ee.FeatureCollection(\"FAO/GAUL/2015/level2\")\n",
    "    .filter(ee.Filter.eq('ADM2_NAME', 'Bangalore Urban'))\n",
    "```\n",
    "\n",
    "### 2. Loading Lake Locations as Point Features\n",
    "\n",
    "```python \n",
    "df = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "features = [\n",
    "    ee.Feature(\n",
    "        ee.Geometry.Point([row['lon'], row['lat']]),\n",
    "        {'name': row['name']}\n",
    "    ) \n",
    "    for i, row in df.iterrows()\n",
    "]\n",
    "lake_points = ee.FeatureCollection(features)\n",
    "```\n",
    "**What is happening**\n",
    "\n",
    "* Reads a CSV file containing lake centroids.\n",
    "\n",
    "* Converts each latitude–longitude pair into:\n",
    "\n",
    "    * An Earth Engine Point geometry\n",
    "\n",
    "    * With lake name as metadata\n",
    "\n",
    "* All points are combined into a FeatureCollection.\n",
    "\n",
    "**Why this matters**\n",
    "* Lake points act as anchors to sample hydrological properties.\n",
    "\n",
    "* Enables point-based queries such as:\n",
    "\n",
    "    * Upstream contributing area\n",
    "\n",
    "    * Flow direction at the lake location\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Loading MERIT Hydro Datasets\n",
    "`merit = ee.Image(\"MERIT/Hydro/v1_0_1\")`\n",
    "\n",
    "**What this dataset is**\n",
    "\n",
    "* **MERIT Hydro** is a globally corrected hydrological dataset.\n",
    "\n",
    "* Built on improved DEMs with:\n",
    "\n",
    "    * Reduced striping errors\n",
    "\n",
    "    * Corrected river networks\n",
    "\n",
    "* It is especially useful for urban flood and drainage analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Extracting Flow Accumulation\n",
    "```python\n",
    "flow_acc = merit.select('upa')\n",
    "flow_acc_viz = flow_acc.log10()\n",
    "```\n",
    "\n",
    "**Key concepts**\n",
    "\n",
    "* `upa` (Upstream Accumulation Area)\n",
    "\n",
    "* Represents the total upstream area draining into each pixel.\n",
    "\n",
    "* High values indicate major drains and valleys.\n",
    "\n",
    "**Why log-transform**\n",
    "\n",
    "* Raw flow accumulation values span several orders of magnitude.\n",
    "\n",
    "* Log transformation:\n",
    "\n",
    "    * Enhances visibility of small urban streams\n",
    "\n",
    "    * Prevents large rivers from dominating the visualization\n",
    "\n",
    "**Hydrological meaning**\n",
    "\n",
    "* Pixels with high values indicate where runoff naturally converges.\n",
    "\n",
    "* Lakes located on high upa pixels are structurally flood-prone.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Extracting Flow Direction\n",
    "`flow_dir = merit.select('dir')`\n",
    "\n",
    "**What dir represents**\n",
    "\n",
    "* Indicates the direction water flows out of each pixel.\n",
    "\n",
    "* Encoded using a D8 flow model (8 possible directions).\n",
    "\n",
    "**Flow direction explains**:\n",
    "\n",
    "* How water moves between lakes\n",
    "\n",
    "* Which lakes are upstream or downstream\n",
    "\n",
    "* Essential for understanding Bengaluru’s historic cascade lake system.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Preparing the Map for Visualisation\n",
    "```python\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(bengaluru_boundary, 11)\n",
    "```\n",
    "\n",
    "**What is happening**\n",
    "\n",
    "* Initializes an interactive map.\n",
    "\n",
    "* Centers the map over Bangalore Urban at a city-scale zoom level.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Flow Accumulation Visualisation Parameters\n",
    "```python\n",
    "acc_params = {\n",
    "    'min': 0, \n",
    "    'max': 5, \n",
    "    'palette': ['#000000', '#023858', '#0570b0', '#74a9cf', '#fff7fb']\n",
    "}\n",
    "```\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "* Dark colors → low or negligible drainage\n",
    "\n",
    "* Light colors → strong drainage pathways\n",
    "\n",
    "**Highlights**:\n",
    "\n",
    "* Natural valleys\n",
    "\n",
    "* Stormwater drains\n",
    "\n",
    "* Low-lying convergence zones\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Flow Direction Visualisation Parameters\n",
    "\n",
    "```python\n",
    "dir_params = {\n",
    "    'min': 1, \n",
    "    'max': 128, \n",
    "    'palette': ['red', 'orange', 'yellow', 'green', 'blue', 'cyan', 'magenta', 'black']\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "* Each color corresponds to a specific flow direction.\n",
    "\n",
    "* Together, they reveal the directional logic of runoff across the city.\n",
    "\n",
    "* Helps visually verify:\n",
    "\n",
    "    * Whether lakes align with natural flow paths\n",
    "\n",
    "    * Where drainage has been disrupted by urban development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get Boundary and Lakes\n",
    "bengaluru_boundary = ee.FeatureCollection(\"FAO/GAUL/2015/level2\") \\\n",
    "    .filter(ee.Filter.eq('ADM2_NAME', 'Bangalore Urban'))\n",
    "\n",
    "# Load your lake points\n",
    "df = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "features = [ee.Feature(ee.Geometry.Point([row['lon'], row['lat']]), {'name': row['name']}) for i, row in df.iterrows()]\n",
    "lake_points = ee.FeatureCollection(features)\n",
    "\n",
    "# 3. Load MERIT Hydro Datasets with CORRECT BAND NAMES\n",
    "merit = ee.Image(\"MERIT/Hydro/v1_0_1\")\n",
    "\n",
    "# 'upa' is the band for Upstream Accumulation Area\n",
    "flow_acc = merit.select('upa') \n",
    "# We log-transform it for better visualization of small streams\n",
    "flow_acc_viz = flow_acc.log10() \n",
    "\n",
    "# 'dir' is the band for Flow Direction\n",
    "flow_dir = merit.select('dir')\n",
    "\n",
    "# 4. Visualization on the Map\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(bengaluru_boundary, 11)\n",
    "\n",
    "# Palette for Flow Accumulation (Blue to White represents the drainage network)\n",
    "acc_params = {\n",
    "    'min': 0, \n",
    "    'max': 5, \n",
    "    'palette': ['#000000', '#023858', '#0570b0', '#74a9cf', '#fff7fb']\n",
    "}\n",
    "\n",
    "# Palette for Direction (Standard 8-direction colors)\n",
    "dir_params = {\n",
    "    'min': 1, \n",
    "    'max': 128, \n",
    "    'palette': ['red', 'orange', 'yellow', 'green', 'blue', 'cyan', 'magenta', 'black']\n",
    "}\n",
    "\n",
    "Map.addLayer(flow_dir.clip(bengaluru_boundary), dir_params, '1. Flow Direction (Compass)')\n",
    "Map.addLayer(flow_acc_viz.clip(bengaluru_boundary), acc_params, '2. Flow Accumulation (Drainage Network)')\n",
    "Map.addLayer(lake_points, {'color': 'red'}, '3. Lake Locations')\n",
    "\n",
    "Map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb245b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Extract and Save Data\n",
    "print(\"Extracting flow stats for lakes...\")\n",
    "# Combine bands into one image for sampling\n",
    "topo_image = ee.Image.cat([\n",
    "    flow_acc.rename('flow_accumulation_km2'),\n",
    "    flow_dir.rename('flow_direction_code')\n",
    "])\n",
    "\n",
    "stats = topo_image.reduceRegions(\n",
    "    collection=lake_points,\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    scale=90\n",
    ")\n",
    "\n",
    "try:\n",
    "    df_results = geemap.ee_to_df(stats)\n",
    "    if not os.path.exists('data'): os.makedirs('data')\n",
    "    df_results.to_csv('data/lake_flow_analysis.csv', index=False)\n",
    "    print(\"Success! Data saved to data/lake_flow_analysis.csv\")\n",
    "    print(df_results[['name', 'flow_accumulation_km2']].sort_values(by='flow_accumulation_km2', ascending=False).head())\n",
    "except Exception as e:\n",
    "    print(f\"Error saving CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b34dfe",
   "metadata": {},
   "source": [
    "## Interpretation of Flow Accumulation and Flow Direction in Bengaluru’s Lakes\n",
    "\n",
    "---\n",
    "\n",
    "### 1. The Giants: Catchment “Masters”\n",
    "\n",
    "#### **Yellamallappa Chetty** and **Bellandur Lake**\n",
    "\n",
    "- These lakes function as the **primary controllers** of their respective catchments.\n",
    "- **Bellandur Lake**, for example, receives runoff from **over 114 km²** of the city.\n",
    "\n",
    "#### **The Bottleneck Effect**\n",
    "\n",
    "- When the **boundaries of a major lake** like **Bellandur** are **encroached**:\n",
    "  - The problem is not limited to the lake itself.\n",
    "  - The entire **114 km² upstream drainage network** begins to **back up**.\n",
    "- This results in **flooding in neighbourhoods located far upstream**, sometimes several kilometres away from the lake.\n",
    "\n",
    "**Key idea**\n",
    "\n",
    "- A large lake acts like a **valve** in the city’s drainage system.\n",
    "- If the valve is blocked, pressure builds everywhere upstream.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Direction of Flow (**flow_direction_code**)\n",
    "\n",
    "The **flow_direction_code** indicates **where water exits a lake pixel**, making it essential for identifying **downstream flood impacts**.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "- **Bellandur Lake (128 → North-East)**\n",
    "  - Water flows toward the **North-East**.\n",
    "  - Ultimately drains toward **Varthur Lake**.\n",
    "\n",
    "- **Ramapura Kere (2 → South-East)**\n",
    "  - Drains toward the **South-East**.\n",
    "\n",
    "- **Anchepalya Lake (32 → North-West)**\n",
    "  - Drains toward the **North-West**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Flow Direction Matters for **Encroachment**\n",
    "\n",
    "- If a lake such as **Yellamallappa Chetty** drains **southward**:\n",
    "  - Heavy **construction and encroachment downstream** block the natural outflow.\n",
    "- With no effective **“exit door”**:\n",
    "  - Water accumulates inside the lake.\n",
    "  - Lake levels rise **much faster** than they would naturally.\n",
    "\n",
    "This transforms moderate rainfall into **sudden urban flooding**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Headwater Lakes: The “Sponges”\n",
    "\n",
    "#### Characteristics\n",
    "\n",
    "- Lakes with **very low flow accumulation**  \n",
    "  - Example: **Chikkabettahalli** (~0.008 km²).\n",
    "- Located at the **very start of drainage lines**.\n",
    "\n",
    "#### Hydrological Role\n",
    "\n",
    "- These lakes:\n",
    "  - Do **not** face high flood risk themselves.\n",
    "  - Play a crucial role in **groundwater recharge**.\n",
    "\n",
    "#### Impact of Encroachment\n",
    "\n",
    "- When headwater lakes are **paved over or filled**:\n",
    "  - Rainwater is no longer held locally.\n",
    "  - The **groundwater table drops sharply**.\n",
    "  - Surrounding areas experience **borewell failure**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Forensic Summary\n",
    "\n",
    "- **High Accumulation + High Encroachment**\n",
    "  - These are **“Flood Bombs”**  \n",
    "  - Example: **Bellandur Lake**\n",
    "  - Capable of triggering **city-scale flooding**.\n",
    "\n",
    "- **Low Accumulation + High Encroachment**\n",
    "  - These become **“Dry Zones”**\n",
    "  - Lakes are destroyed quietly.\n",
    "  - Result is **chronic water scarcity**, not floods.\n",
    "\n",
    "---\n",
    "\n",
    "### One-line Insight\n",
    "\n",
    "**Floods and water scarcity in Bengaluru are two sides of the same problem: the destruction of lakes that once regulated flow at different points in the drainage hierarchy.**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3ad78",
   "metadata": {},
   "source": [
    "## Why Many Lakes Show a Very Small Flow Accumulation Value (~0.008 km²)\n",
    "\n",
    "Several lakes in your analysis show a **tiny and repeated flow accumulation value** (≈ **0.00833 km²**).  \n",
    "This is not an error — it reflects **how hydrological data, terrain position, and geometry alignment interact**.  \n",
    "The reasons are explained below.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The **“Single Pixel” Limit** (Dataset Resolution)\n",
    "\n",
    "The **MERIT Hydro** dataset used in the script has a spatial resolution of:\n",
    "\n",
    "- **90 m × 90 m per pixel**\n",
    "\n",
    "### Pixel-area logic\n",
    "\n",
    "- Area of one pixel  \n",
    "  - **90 m × 90 m = 8,100 m²**\n",
    "- Converted to square kilometres  \n",
    "  - **0.0081 km²**\n",
    "- Adjusted for Earth’s curvature at Bengaluru’s latitude  \n",
    "  - **≈ 0.00833 km²**\n",
    "\n",
    "### What this means in practice\n",
    "\n",
    "- When a lake shows **~0.00833 km²**:\n",
    "  - The algorithm found **no upstream pixels draining into it**\n",
    "  - It is only counting the **single pixel** on which the lake point is located\n",
    "\n",
    "**Key interpretation**\n",
    "\n",
    "- **Zero upstream contribution**\n",
    "- Only **local pixel area** is being registered\n",
    "\n",
    "---\n",
    "\n",
    "## 2. These Are **Headwater Lakes** (The Ridge Effect)\n",
    "\n",
    "### Bengaluru’s terrain context\n",
    "\n",
    "- The city is built on a series of **ridges and shallow plateaus**\n",
    "- Water flows:\n",
    "  - From **ridges → valleys**\n",
    "  - From **headwaters → downstream sinks**\n",
    "\n",
    "### Position of many lakes\n",
    "\n",
    "Lakes such as:\n",
    "\n",
    "- **Agara Lake**\n",
    "- **Chikkabettahalli**\n",
    "- **Yelahanka**\n",
    "\n",
    "sit at the **very top of these ridges**.\n",
    "\n",
    "### Hydrological consequence\n",
    "\n",
    "- There is **no land physically higher** than these lakes\n",
    "- Therefore:\n",
    "  - **No upstream catchment exists**\n",
    "  - No flow accumulation can be added\n",
    "\n",
    "### **Forensic insight**\n",
    "\n",
    "- These lakes are the **origins of Bengaluru’s drainage system**\n",
    "- They rely almost entirely on:\n",
    "  - **Direct rainfall**\n",
    "  - Not upstream runoff\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Point vs. Drain Alignment** (Geometry Mismatch)\n",
    "\n",
    "### What the script currently does\n",
    "\n",
    "- Uses **lake_points** (single coordinate points)\n",
    "- Samples flow accumulation at **exact pixel locations**\n",
    "\n",
    "### The alignment problem\n",
    "\n",
    "- **Rajakaluves (stormwater drains)** are often:\n",
    "  - Narrow\n",
    "  - One or two pixels wide\n",
    "- If a lake point is even **30–50 metres away** from the centre of the drain pixel:\n",
    "  - The algorithm thinks the lake is on **adjacent dry land**\n",
    "  - Not inside the drainage line\n",
    "\n",
    "### Result\n",
    "\n",
    "- The script reports:\n",
    "  - Only the **single-pixel area (0.00833 km²)**\n",
    "- Meanwhile:\n",
    "  - A **large flow** may exist just **one pixel away**, but is ignored\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters for Your Study\n",
    "\n",
    "### **“Zero” Accumulation Lakes = Sponges**\n",
    "\n",
    "- Lakes with near-zero accumulation:\n",
    "  - Are **headwater lakes**\n",
    "  - Play a **critical role in groundwater recharge**\n",
    "- Their main function:\n",
    "  - **Absorb rainfall locally**\n",
    "- Encroachment here leads to:\n",
    "  - Falling groundwater tables\n",
    "  - Borewell failure\n",
    "\n",
    "---\n",
    "\n",
    "### **High Accumulation Lakes = Sinks**\n",
    "\n",
    "- Lakes like **Bellandur** (~**114 km²**):\n",
    "  - Receive runoff from **huge upstream areas**\n",
    "  - Act as **terminal collectors**\n",
    "- They are:\n",
    "  - Highly flood-prone\n",
    "  - Extremely sensitive to encroachment and blockage\n",
    "\n",
    "---\n",
    "\n",
    "## How to Get More **Realistic Catchment Numbers**\n",
    "\n",
    "### Problem with current approach\n",
    "\n",
    "- Sampling only **exact point locations**\n",
    "- Misses nearby high-flow pixels\n",
    "\n",
    "### Solution: **Buffered Sampling**\n",
    "\n",
    "- Replace point sampling with:\n",
    "  - **`buffer(200)`** around each lake point\n",
    "\n",
    "### Why this works\n",
    "\n",
    "- Allows the algorithm to:\n",
    "  - “Reach out” to the nearest **high-flow drain pixel**\n",
    "  - Capture the **true contributing catchment**\n",
    "- Especially useful for lakes that:\n",
    "  - Sit slightly off the mapped drain centerline\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "**Tiny accumulation values do not mean lakes are unimportant.**  \n",
    "They identify **headwater “sponge” lakes**, which are crucial for groundwater stability, while **large accumulation values identify flood-critical sink lakes** like Bellandur.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get Data\n",
    "bengaluru_boundary = ee.FeatureCollection(\"FAO/GAUL/2015/level2\") \\\n",
    "    .filter(ee.Filter.eq('ADM2_NAME', 'Bangalore Urban'))\n",
    "\n",
    "# Load lake points from your previous mean CSV\n",
    "df = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "features = [ee.Feature(ee.Geometry.Point([row['lon'], row['lat']]), {'name': row['name']}) for i, row in df.iterrows()]\n",
    "lake_points = ee.FeatureCollection(features)\n",
    "\n",
    "# 3. Create the Sink Depth Layer\n",
    "# Raw Elevation (SRTM)\n",
    "raw_elevation = ee.Image(\"USGS/SRTMGL1_003\").select('elevation')\n",
    "\n",
    "# Filled Elevation (MERIT Hydro 'elv' band is hydro-conditioned/filled)\n",
    "filled_dem = ee.Image(\"MERIT/Hydro/v1_0_1\").select('elv')\n",
    "\n",
    "# Sink Depth = Filled - Raw\n",
    "# If the result is positive, it means a \"hole\" was filled by that many meters.\n",
    "depressions = filled_dem.subtract(raw_elevation).rename('sink_depth')\n",
    "\n",
    "# Remove noise: only keep areas where the fill is more than 0.5 meters\n",
    "depressions_masked = depressions.updateMask(depressions.gt(0.5))\n",
    "\n",
    "# 4. Visualization\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(bengaluru_boundary, 11)\n",
    "\n",
    "sink_viz = {\n",
    "    'min': 0, \n",
    "    'max': 10, \n",
    "    'palette': ['white', 'yellow', 'orange', 'red'] # Red = Deepest depressions\n",
    "}\n",
    "\n",
    "Map.addLayer(raw_elevation.clip(bengaluru_boundary), {'min': 800, 'max': 1000}, '1. Raw Elevation')\n",
    "Map.addLayer(depressions_masked.clip(bengaluru_boundary), sink_viz, '2. Detected Depressions (Sinks)')\n",
    "Map.addLayer(lake_points, {'color': 'blue'}, '3. Lake Points')\n",
    "\n",
    "Map\n",
    "\n",
    "# 5. Export results\n",
    "print(\"Calculating sink depth for lakes...\")\n",
    "stats = depressions.reduceRegions(\n",
    "    collection=lake_points,\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    scale=30\n",
    ")\n",
    "\n",
    "df_results = geemap.ee_to_df(stats)\n",
    "if not os.path.exists('data'): os.makedirs('data')\n",
    "df_results.to_csv('data/lake_sink_analysis.csv', index=False)\n",
    "print(\"Success! Saved to data/lake_sink_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c350b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Initialize\n",
    "ee.Initialize(project='bengaluru-lakes-485612')\n",
    "\n",
    "# Load Lake Points\n",
    "df = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "features = [ee.Feature(ee.Geometry.Point([row['lon'], row['lat']]), {'name': row['name']}) for i, row in df.iterrows()]\n",
    "lake_points = ee.FeatureCollection(features)\n",
    "\n",
    "# 2. LOAD BOTH GENERATIONS OF TOPOGRAPHY\n",
    "# Historical (Year 2000)\n",
    "srtm = ee.Image(\"USGS/SRTMGL1_003\").select('elevation')\n",
    "\n",
    "# Modern (Year 2021 Snapshot - Copernicus GLO-30)\n",
    "# This is the most accurate representation of Bengaluru's current terrain\n",
    "copernicus = ee.ImageCollection(\"COPERNICUS/DEM/GLO30\").mosaic().select('DEM')\n",
    "\n",
    "# 3. CALCULATE SINK DEPTH (FILLING) FOR BOTH PERIODS\n",
    "# We use MERIT Hydro as the \"Hydrologically Correct\" reference\n",
    "filled_ref = ee.Image(\"MERIT/Hydro/v1_0_1\").select('elv')\n",
    "\n",
    "sink_2000 = filled_ref.subtract(srtm).rename('sink_depth_2000')\n",
    "sink_2021 = filled_ref.subtract(copernicus).rename('sink_depth_2021')\n",
    "\n",
    "# 4. CALCULATE DYNAMIC PRESSURE (Runoff Sum for 2020-2024)\n",
    "# Fixing the band name to 'surface_runoff_sum'\n",
    "runoff_2020_2024 = ee.ImageCollection(\"ECMWF/ERA5_LAND/MONTHLY_AGGR\") \\\n",
    "    .filterDate('2020-01-01', '2024-12-31') \\\n",
    "    .select('surface_runoff_sum') \\\n",
    "    .sum() \\\n",
    "    .rename('total_runoff_20s')\n",
    "\n",
    "# 5. EXTRACT DATA\n",
    "combined_stats = sink_2000.addBands(sink_2021).addBands(runoff_2020_2024)\n",
    "\n",
    "results = combined_stats.reduceRegions(\n",
    "    collection=lake_points,\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    scale=30\n",
    ")\n",
    "\n",
    "# 6. EXPORT\n",
    "df_final = geemap.ee_to_df(results)\n",
    "# Calculate the \"Filling Rate\"\n",
    "df_final['depth_change'] = df_final['sink_depth_2021'] - df_final['sink_depth_2000']\n",
    "\n",
    "df_final.to_csv('data/lake_topographic_change_2000_2025.csv', index=False)\n",
    "print(\"Comparison complete! Check 'lake_topographic_change_2000_2025.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Initialize\n",
    "ee.Initialize(project='bengaluru-lakes-485612')\n",
    "\n",
    "# 2. Load Topography\n",
    "copernicus = ee.ImageCollection(\"COPERNICUS/DEM/GLO30\").mosaic().select('DEM')\n",
    "\n",
    "# Load Lake Points and BUFFER them by 100m to create small polygons\n",
    "df = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "features = [\n",
    "    ee.Feature(ee.Geometry.Point([row['lon'], row['lat']]).buffer(100), {'name': row['name']}) \n",
    "    for i, row in df.iterrows()\n",
    "]\n",
    "lake_polygons = ee.FeatureCollection(features)\n",
    "\n",
    "def calculate_bathymetry(year):\n",
    "    # Sentinel-2 Median for the year (broadened window to ensure water capture)\n",
    "    s2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n",
    "        .filterDate(f'{year}-01-01', f'{year}-12-31') \\\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
    "        .median()\n",
    "    \n",
    "    # Relaxed Water Mask (includes shallow/weedy water)\n",
    "    ndwi = s2.normalizedDifference(['B3', 'B8'])\n",
    "    water_mask = ndwi.gt(0.0) # Lowered threshold from 0.1 to 0.0\n",
    "    \n",
    "    # Step A: Find the 'Rim Elevation' (Max elevation in the 100m buffer)\n",
    "    # This is more accurate for urban lakes than MERIT\n",
    "    rim_elevation = copernicus.reduceRegions(\n",
    "        collection=lake_polygons,\n",
    "        reducer=ee.Reducer.max(),\n",
    "        scale=30\n",
    "    ).reduceToImage(properties=['max'], engine='mean')\n",
    "\n",
    "    # Step B: Depth = Rim - Ground\n",
    "    # Only calculate where there is water detected by Sentinel-2\n",
    "    depth_image = rim_elevation.subtract(copernicus).updateMask(water_mask).rename('depth')\n",
    "    \n",
    "    # Step C: Volume (Area * Depth)\n",
    "    volume_image = depth_image.multiply(ee.Image.pixelArea()).rename('volume')\n",
    "    \n",
    "    stats = depth_image.addBands(volume_image).reduceRegions(\n",
    "        collection=lake_polygons,\n",
    "        reducer=ee.Reducer.mean().combine(\n",
    "            reducer2=ee.Reducer.sum(), sharedInputs=True\n",
    "        ),\n",
    "        scale=10\n",
    "    )\n",
    "    return stats\n",
    "\n",
    "# 3. Execution\n",
    "all_years = []\n",
    "for year in range(2020, 2026):\n",
    "    print(f\"Processing {year}...\")\n",
    "    try:\n",
    "        yearly_df = geemap.ee_to_df(calculate_bathymetry(year))\n",
    "        yearly_df['year'] = year\n",
    "        all_years.append(yearly_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {year}: {e}\")\n",
    "\n",
    "# 4. Final Export\n",
    "final_df = pd.concat(all_years)\n",
    "final_df.to_csv('data/lake_bathymetry_verified_2020_2025.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "\n",
    "# 1. Initialize Earth Engine\n",
    "ee.Initialize(project='bengaluru-lakes-485612')\n",
    "\n",
    "# 2. Define Area of Interest (Bengaluru)\n",
    "# Replace with your specific lake coordinates or boundary\n",
    "roi = ee.Geometry.Point([77.5946, 12.9716]).buffer(30000) \n",
    "\n",
    "# 3. Load Sentinel-1 SAR Collection\n",
    "# Filter for IW mode, VH polarization (better for urban water detection)\n",
    "s1_collection = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "    .filterBounds(roi) \\\n",
    "    .filterDate('2020-01-01', '2025-12-31') \\\n",
    "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH')) \\\n",
    "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "\n",
    "# 4. Define the Water Threshold\n",
    "# -20 dB is a standard threshold for VH. Values below this are likely water.\n",
    "def identify_water(image):\n",
    "    # Apply threshold\n",
    "    water = image.select('VH').lt(-20).rename('is_water')\n",
    "    # Return 1 for water, 0 for land, masked for invalid data\n",
    "    return water.copyProperties(image, ['system:time_start'])\n",
    "\n",
    "# 5. Map the function over the collection\n",
    "water_time_series = s1_collection.map(identify_water)\n",
    "\n",
    "# 6. Calculate Frequency\n",
    "# Frequency = (Sum of water detections / Total number of observations) * 100\n",
    "total_observations = water_time_series.count()\n",
    "water_observations = water_time_series.sum()\n",
    "\n",
    "flood_frequency = water_observations.divide(total_observations).multiply(100).rename('flood_freq_pct')\n",
    "\n",
    "# 7. Visualization\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(roi, 12)\n",
    "\n",
    "# Set visualization parameters: 0% (Dry) is transparent/white, 100% (Permanent Water) is Blue\n",
    "viz_params = {\n",
    "    'min': 0,\n",
    "    'max': 100,\n",
    "    'palette': ['#ffffff', '#ff0000', '#0000ff'] # Red for occasional flooding, Blue for permanent\n",
    "}\n",
    "\n",
    "Map.addLayer(flood_frequency.clip(roi), viz_params, 'SAR Flood Frequency (%)')\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35e0d1",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Lake-wise SAR (Synthetic Aperture Radar) Flood Frequency Extraction\n",
    "\n",
    "This script computes **observed flood frequency** around each Bengaluru lake using **Sentinel-1 SAR radar data** for the period **2020–2025**. The output is a lake-level dataset showing **how often flooding was actually detected**, based on satellite observations rather than model assumptions.\n",
    "\n",
    "---\n",
    "\n",
    "**Initialization and Data Loading**\n",
    "\n",
    "- Earth Engine is initialized so all geospatial processing runs on Google’s servers.\n",
    "- A CSV containing lake names and coordinates is loaded into a Pandas DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "**Building a City-wide Flood Frequency Image (Done Once)**\n",
    "\n",
    "- All lake coordinates are combined into a single **MultiPoint geometry**, then buffered by **500 m** to define a city-wide region of interest.\n",
    "- Sentinel-1 SAR images are loaded and filtered:\n",
    "  - Spatially: only images covering the buffered lake region\n",
    "  - Temporally: 2020–2025\n",
    "  - Polarisation: **VH** (best for water detection in urban areas)\n",
    "  - Mode: **IW** (standard land observation mode)\n",
    "\n",
    "- Each SAR image is converted into a **binary water map** using a −20 dB threshold:\n",
    "  - Values below −20 dB → likely water/flooded\n",
    "  - Values above −20 dB → land or built-up\n",
    "- All binary water maps are stacked over time.\n",
    "- **Flood frequency (%)** is computed per pixel as:  \n",
    "  *(number of times water was detected ÷ number of observations) × 100*  \n",
    "  This produces a single raster (`sar_flood_freq_pct`) showing how often each pixel was inundated over five years.\n",
    "\n",
    "---\n",
    "\n",
    "**Lake-wise Extraction Loop**\n",
    "\n",
    "- The script loops through lakes **client-side (Python)** for progress monitoring.\n",
    "- For each lake:\n",
    "  - A **200 m buffer** around the lake centroid is created to capture spillover and nearby waterlogging.\n",
    "  - The **mean flood frequency** within this buffer is extracted from the precomputed SAR flood-frequency image using `reduceRegion`.\n",
    "  - This yields one number per lake:  \n",
    "    *“On average, what percentage of satellite passes detected flooding here?”*\n",
    "\n",
    "- Results are stored in a list with lake name and flood frequency.\n",
    "- Errors for individual lakes are caught so the loop continues uninterrupted.\n",
    "\n",
    "---\n",
    "\n",
    "**Saving the Output**\n",
    "\n",
    "- The collected results are converted to a DataFrame.\n",
    "- A CSV file is written containing:\n",
    "  - `name` → lake name  \n",
    "  - `sar_flood_freq_pct` → observed flood frequency (2020–2025)\n",
    "\n",
    "---\n",
    "\n",
    "**Conceptual Meaning**\n",
    "\n",
    "- This workflow provides an **empirical, observation-based measure of flooding**, not a simulated one.\n",
    "- It captures:\n",
    "  - chronic waterlogging\n",
    "  - repeated lake overflows\n",
    "  - drainage failures\n",
    "- The output is ideal for:\n",
    "  - validating flood models\n",
    "  - identifying flood hotspots\n",
    "  - serving as a **ground-truth target** for machine-learning flood-risk models\n",
    "\n",
    "---\n",
    "\n",
    "**One-line takeaway**\n",
    "\n",
    "This code converts five years of Sentinel-1 radar imagery into a lake-wise measure of how often flooding actually occurred around Bengaluru’s lakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef4c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 1. Initialize\n",
    "ee.Initialize(project='bengaluru-lakes-485612')\n",
    "\n",
    "# 2. Load Data\n",
    "df_lakes = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "\n",
    "# 3. Create the Base Frequency Image (Do this ONCE outside the loop)\n",
    "roi_all = ee.Geometry.MultiPoint(df_lakes[['lon', 'lat']].values.tolist()).buffer(500)\n",
    "s1_collection = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "    .filterBounds(roi_all) \\\n",
    "    .filterDate('2020-01-01', '2025-12-31') \\\n",
    "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH')) \\\n",
    "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "\n",
    "def identify_water(image):\n",
    "    return image.select('VH').lt(-20).rename('is_water').copyProperties(image, ['system:time_start'])\n",
    "\n",
    "water_ts = s1_collection.map(identify_water)\n",
    "flood_freq_img = water_ts.sum().divide(water_ts.count()).multiply(100).rename('sar_flood_freq_pct')\n",
    "\n",
    "# 4. Processing Loop with Prints\n",
    "results = []\n",
    "print(f\"Starting extraction for {len(df_lakes)} lakes...\")\n",
    "\n",
    "for index, row in df_lakes.iterrows():\n",
    "    lake_name = row['name']\n",
    "    print(f\"[{index+1}/{len(df_lakes)}] Processing: {lake_name}...\", end=\"\\r\")\n",
    "    \n",
    "    # Define local geometry\n",
    "    point = ee.Geometry.Point([row['lon'], row['lat']]).buffer(200)\n",
    "    \n",
    "    # Extract mean frequency for this specific lake\n",
    "    try:\n",
    "        # reduceRegion (singular) is faster for a single geometry\n",
    "        stat = flood_freq_img.reduceRegion(\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            geometry=point,\n",
    "            scale=10,\n",
    "            maxPixels=1e9\n",
    "        ).getInfo()\n",
    "        \n",
    "        results.append({\n",
    "            'name': lake_name,\n",
    "            'sar_flood_freq_pct': stat.get('sar_flood_freq_pct')\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {lake_name}: {e}\")\n",
    "\n",
    "# 5. Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('data/lake_sar_flood_frequency_2025.csv', index=False)\n",
    "print(\"\\nExtraction Complete! File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a001da3",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Measuring Rainfall Intensity and Timing\n",
    "* For urban flooding in Bengaluru, \"**Total Rainfall**\" is less important than \"**Intensity**\" (how much rain falls in a short window). \n",
    "* We use the **GPM (Global Precipitation Measurement) IMERG dataset**, which provides data every 30 minutes.\n",
    "    * **Metric 1 (Intensity): Max Daily Rainfall (mm/day)**.\n",
    "    * **Metric 2 (Timing)**: The month of the peak rainfall event (to correlate with your SAR flood observations).\n",
    "\n",
    "---\n",
    "\n",
    "## Measuring Imperviousness\n",
    "* \"Imperviousness\" refers to surfaces like concrete, asphalt, and rooftops that prevent water from soaking into the ground. \n",
    "* A high impervious percentage in the **200m buffer** around a lake leads to rapid runoff and higher flood risk.\n",
    "    * Dataset: **Dynamic World (10m) or ESA WorldCover**. Dynamic World is preferred because it's at **10m resolution (same as Sentinel-2).**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0315ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Initialize\n",
    "ee.Initialize(project='bengaluru-lakes-485612')\n",
    "\n",
    "# 2. Load Geometries\n",
    "df_lakes = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "features = [\n",
    "    ee.Feature(ee.Geometry.Point([row['lon'], row['lat']]).buffer(200), {'name': row['name']}) \n",
    "    for _, row in df_lakes.iterrows()\n",
    "]\n",
    "lake_fc = ee.FeatureCollection(features)\n",
    "\n",
    "def export_hydrology_year(year):\n",
    "    print(f\"Submitting Task for {year}...\")\n",
    "    start_date = ee.Date.fromYMD(year, 1, 1)\n",
    "    end_date = ee.Date.fromYMD(year, 12, 31)\n",
    "\n",
    "    # --- 1. RAINFALL: DAILY AGGREGATION ---\n",
    "    gpm = ee.ImageCollection(\"NASA/GPM_L3/IMERG_V07\") \\\n",
    "        .filterDate(start_date, end_date) \\\n",
    "        .select('precipitation')\n",
    "\n",
    "    days = ee.List.sequence(0, end_date.difference(start_date, 'day').subtract(1))\n",
    "    \n",
    "    def calc_daily(d):\n",
    "        date = start_date.advance(d, 'day')\n",
    "        return gpm.filterDate(date, date.advance(1, 'day')) \\\n",
    "                  .sum().multiply(0.5) \\\n",
    "                  .set('system:time_start', date.millis())\n",
    "    \n",
    "    daily_col = ee.ImageCollection.fromImages(days.map(calc_daily))\n",
    "    daily_list = daily_col.toList(366)\n",
    "\n",
    "    # --- 2. VECTORIZED ROLLING 3-DAY SUM (Faster) ---\n",
    "    # We sum Image(i) + Image(i-1) + Image(i-2)\n",
    "    indices = ee.List.sequence(2, daily_list.length().subtract(1))\n",
    "    \n",
    "    def sum_3days(i):\n",
    "        i = ee.Number(i)\n",
    "        img1 = ee.Image(daily_list.get(i))\n",
    "        img2 = ee.Image(daily_list.get(i.subtract(1)))\n",
    "        img3 = ee.Image(daily_list.get(i.subtract(2)))\n",
    "        return img1.add(img2).add(img3).set('system:time_start', img1.get('system:time_start'))\n",
    "\n",
    "    max_3day_img = ee.ImageCollection.fromImages(indices.map(sum_3days)).max().rename('max_3day_rain_mm')\n",
    "\n",
    "    # --- 3. PEAK INTENSITY & IMPERVIOUSNESS ---\n",
    "    peak_30min_img = gpm.max().multiply(0.5).rename('peak_30min_intensity_mm')\n",
    "    \n",
    "    dw = ee.ImageCollection(\"GOOGLE/DYNAMICWORLD/V1\") \\\n",
    "        .filterDate(start_date, end_date).select('label').mode()\n",
    "    impervious_img = dw.eq(6).rename('impervious_fraction')\n",
    "\n",
    "    # --- 4. BATCH EXTRACTION ---\n",
    "    combined = peak_30min_img.addBands([max_3day_img, impervious_img])\n",
    "    \n",
    "    stats = combined.reduceRegions(\n",
    "        collection=lake_fc,\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        scale=10,\n",
    "        tileScale=4 # Splits the job into smaller tiles to avoid memory errors\n",
    "    )\n",
    "\n",
    "    # --- 5. EXPORT TO DRIVE ---\n",
    "    task = ee.batch.Export.table.toDrive(\n",
    "        collection=stats,\n",
    "        description=f'Hydrology_Stats_{year}',\n",
    "        folder='EE_Exports', # Folder name in your Google Drive\n",
    "        fileNamePrefix=f'lake_stats_{year}',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    task.start()\n",
    "\n",
    "# Run for all years\n",
    "for yr in range(2020, 2026):\n",
    "    export_hydrology_year(yr)\n",
    "\n",
    "print(\"All tasks submitted! Check your Google Earth Engine 'Tasks' tab or your Google Drive 'EE_Exports' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b577e",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Recorded data cleaning for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911606e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Find all files starting with 'lake_stats_' in the data folder\n",
    "files = sorted(glob.glob('data/lake_stats_20*.csv'))\n",
    "\n",
    "all_years = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    # Optional: If the CSV doesn't have a 'year' column, extract it from the filename\n",
    "    if 'year' not in df.columns:\n",
    "        year = f.split('_')[-1].replace('.csv', '')\n",
    "        df['year'] = int(year)\n",
    "    all_years.append(df)\n",
    "# Merge everything\n",
    "master_df = pd.concat(all_years, ignore_index=True)\n",
    "\n",
    "cols_to_drop = ['system:index', '.geo']\n",
    "master_df = master_df.drop(columns=[c for c in cols_to_drop if c in master_df.columns])\n",
    "master_df = master_df.sort_values(by = ['name', 'year'])\n",
    "master_df = master_df[['name', 'impervious_fraction', 'max_3day_rain_mm', 'peak_30min_intensity_mm', 'year']]\n",
    "\n",
    "print(f\"Merged {len(files)} files into a single master DataFrame.\")\n",
    "\n",
    "master_df.to_csv('data/lake_stats_summary_2020_2025.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b32164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load all datasets\n",
    "df_hydro = pd.read_csv('data/lake_stats_summary_2020_2025.csv')\n",
    "df_landuse = pd.read_csv('data/bengaluru_lakes_cleaned_gt_0.5ha.csv')\n",
    "df_flow = pd.read_csv('data/lake_flow_analysis.csv')\n",
    "df_flood = pd.read_csv('data/lake_sar_flood_frequency_2025.csv')\n",
    "df_encroach = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "\n",
    "# 2. Average the Yearly Data (Hydro & Land Use)\n",
    "# We drop 'year' and 'Unnamed: 0' before averaging\n",
    "hydro_mean = df_hydro.drop(columns=['year', 'Unnamed: 0'], errors='ignore').groupby('name').mean().reset_index()\n",
    "\n",
    "# For landuse, we keep lat/lon as they are constant, but average the areas\n",
    "landuse_mean = df_landuse.drop(columns=['year', 'Unnamed: 0'], errors='ignore').groupby('name').mean().reset_index()\n",
    "\n",
    "# 3. Merge into a single \"Representative\" DataFrame\n",
    "# Start with landuse_mean as it contains lat/lon\n",
    "ml_dataset = pd.merge(landuse_mean, hydro_mean, on='name', how='inner')\n",
    "\n",
    "# Add static flow data\n",
    "ml_dataset = pd.merge(ml_dataset, df_flow, on='name', how='left')\n",
    "\n",
    "# Add pre-calculated encroachment data\n",
    "ml_dataset = pd.merge(ml_dataset, df_encroach[['name', 'encroachment_pct']], on='name', how='left')\n",
    "\n",
    "# Add the TARGET variable (Flood Frequency)\n",
    "ml_dataset = pd.merge(ml_dataset, df_flood, on='name', how='left')\n",
    "\n",
    "# 4. Final Cleanup\n",
    "ml_dataset.fillna(0, inplace=True)\n",
    "\n",
    "# 5. Save for ML\n",
    "ml_dataset.to_csv('data/lake_flood_ml_ready.csv', index=False)\n",
    "\n",
    "print(f\"ML Dataset Created: {ml_dataset.shape[0]} lakes and {ml_dataset.shape[1]} features.\")\n",
    "print(\"Sample of predictors:\", ml_dataset[['name', 'impervious_fraction', 'flow_accumulation_km2', 'sar_flood_freq_pct']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f718418",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### ML–Based Flood Risk Classification\n",
    "\n",
    "This script builds and evaluates a **lake-level flood risk classification model** for Bengaluru using **observed SAR flood frequency** as the outcome and a set of **physically meaningful flood drivers** as predictors. The goal is to classify lakes into **Low Risk** and **High Risk** flood categories in a way that is interpretable and actionable.\n",
    "\n",
    "---\n",
    "\n",
    "**Data Loading**\n",
    "\n",
    "- A pre-processed, lake-level dataset (`lake_flood_ml_ready.csv`) is loaded.\n",
    "- Each row represents one lake, with rainfall, land-cover, drainage, and observed flood-frequency metrics already aggregated spatially.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Selection: The Three Pillars Framework**\n",
    "\n",
    "- **Hydrological Drivers (Trigger):**\n",
    "  - `max_3day_rain_mm` → cumulative wetness / system saturation  \n",
    "  - `peak_30min_intensity_mm` → short-duration storm intensity\n",
    "\n",
    "- **Land-Cover Vulnerability (Resistance):**\n",
    "  - `impervious_fraction` → runoff efficiency  \n",
    "  - `in_build_ha` → built-up pressure near lakes  \n",
    "  - `encroachment_pct` → loss of natural buffer and storage\n",
    "\n",
    "- **Landscape Topology (Gravity):**\n",
    "  - `flow_accumulation_km2` → upstream drainage pressure  \n",
    "  - `potential_ha` → lake basin scale\n",
    "\n",
    "These features reflect **physical flood processes**, not just statistical convenience.\n",
    "\n",
    "---\n",
    "\n",
    "**Target Variable Construction**\n",
    "\n",
    "- The continuous SAR-derived flood frequency (`sar_flood_freq_pct`) is converted into a binary risk label.\n",
    "- Lakes with flood frequency **greater than 25%** are labelled as **High Risk** (`1`); others as **Low Risk** (`0`).\n",
    "- This threshold produces a policy-friendly flood-risk classification while preserving an observational basis.\n",
    "\n",
    "---\n",
    "\n",
    "**Data Cleaning and Train–Test Split**\n",
    "\n",
    "- Rows with missing feature or label values are removed.\n",
    "- The dataset is split into:\n",
    "  - **80% training data**\n",
    "  - **20% testing data**\n",
    "- A fixed random seed ensures reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Model Training**\n",
    "\n",
    "- A **Random Forest Classifier** with 100 decision trees is trained.\n",
    "- Random Forests are well suited here because:\n",
    "  - flood drivers interact non-linearly\n",
    "  - features operate at different scales\n",
    "  - the model remains interpretable via feature importance\n",
    "\n",
    "---\n",
    "\n",
    "**Model Evaluation**\n",
    "\n",
    "- Predictions are generated for the test set.\n",
    "- Performance is assessed using:\n",
    "  - **Accuracy** → overall correctness\n",
    "  - **Classification report** → precision, recall, and F1-score for Low and High Risk classes\n",
    "- This evaluates how well physical drivers explain observed flooding.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Importance Analysis**\n",
    "\n",
    "- The contribution of each feature to the model’s decisions is extracted.\n",
    "- Features are grouped by category (Rain, Buildings, Topology) to assess:\n",
    "  - which physical processes dominate flood risk\n",
    "- A bar plot visualizes relative importance for intuitive interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "**Saving Final Predictions**\n",
    "\n",
    "- Model predictions are mapped back to lake names.\n",
    "- The output CSV contains:\n",
    "  - lake name\n",
    "  - observed flood frequency\n",
    "  - true risk label\n",
    "  - predicted risk class\n",
    "- This enables direct comparison between observed and modelled flood risk.\n",
    "\n",
    "---\n",
    "\n",
    "**Conceptual Meaning**\n",
    "\n",
    "- This workflow translates **observed flooding patterns** into a **predictive, interpretable risk classification**.\n",
    "- It does not simulate floods; instead, it learns which combinations of rainfall, urbanisation, and drainage characteristics are associated with repeated inundation.\n",
    "- The results are suitable for:\n",
    "  - prioritising flood-prone lakes\n",
    "  - policy and planning discussions\n",
    "  - downstream regression or risk-index development\n",
    "\n",
    "---\n",
    "\n",
    "**One-line takeaway**\n",
    "\n",
    "This code uses a Random Forest classifier to learn how rainfall, urban encroachment, and drainage topology jointly determine whether Bengaluru’s lakes are repeatedly flood-prone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d05e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# 1. LOAD DATA\n",
    "# Ensure you are using the lake-level averaged dataset\n",
    "df = pd.read_csv('data/lake_flood_ml_ready.csv')\n",
    "\n",
    "# 2. FEATURE SELECTION (The Three Pillars)\n",
    "# Hydrological Drivers (The Trigger)\n",
    "rain_features = ['max_3day_rain_mm', 'peak_30min_intensity_mm']\n",
    "# Land-Cover Vulnerability (The Resistance)\n",
    "building_features = ['impervious_fraction', 'in_build_ha', 'encroachment_pct']\n",
    "# Landscape Topology (The Gravity)\n",
    "topology_features = ['flow_accumulation_km2', 'potential_ha']\n",
    "\n",
    "X_features = rain_features + building_features + topology_features\n",
    "target = 'sar_flood_freq_pct'\n",
    "\n",
    "# 3. PREPARE CLASSIFICATION TARGET\n",
    "# We define \"High Risk\" as any lake with > 25% flood frequency\n",
    "threshold = 25\n",
    "df['risk_label'] = (df[target] > threshold).astype(int)\n",
    "\n",
    "# 4. DATA CLEANING & SPLITTING\n",
    "df_ml = df.dropna(subset=X_features + ['risk_label'])\n",
    "X = df_ml[X_features]\n",
    "y = df_ml['risk_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. TRAIN RANDOM FOREST CLASSIFIER\n",
    "# We use a classifier to maximize accuracy and actionable insights\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. EVALUATION\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"--- Model Performance ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))\n",
    "\n",
    "# 7. FEATURE IMPORTANCE ANALYSIS\n",
    "importances = model.feature_importances_\n",
    "feat_df = pd.DataFrame({\n",
    "    'Feature': X_features,\n",
    "    'Importance': importances,\n",
    "    'Category': (['Rain'] * 2) + (['Buildings'] * 3) + (['Topology'] * 2)\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# 8. VISUALIZATION\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feat_df, x='Importance', y='Feature', hue='Category', dodge=False)\n",
    "plt.title('Drivers of Flood Risk in Bengaluru (ML Feature Importance)')\n",
    "plt.xlabel('Contribution to Model Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Category-level Importance\n",
    "cat_importance = feat_df.groupby('Category')['Importance'].sum().sort_values(ascending=False)\n",
    "print(\"\\n--- Importance by Category ---\")\n",
    "print(cat_importance)\n",
    "\n",
    "# 9. SAVE FINAL PREDICTIONS\n",
    "# Map predictions back to lake names for the test set\n",
    "results = df_ml.loc[X_test.index, ['name', target, 'risk_label']].copy()\n",
    "results['predicted_risk'] = y_pred\n",
    "results.to_csv('final_flood_risk_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61919b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 1. LOAD DATA\n",
    "df = pd.read_csv('data/lake_flood_ml_ready.csv')\n",
    "\n",
    "# 2. FEATURE SELECTION (Same three pillars)\n",
    "rain_features = ['max_3day_rain_mm', 'peak_30min_intensity_mm']\n",
    "building_features = ['impervious_fraction', 'in_build_ha', 'encroachment_pct']\n",
    "topology_features = ['flow_accumulation_km2', 'potential_ha']\n",
    "\n",
    "X_features = rain_features + building_features + topology_features\n",
    "target = 'sar_flood_freq_pct'\n",
    "\n",
    "# 3. CLEAN DATA\n",
    "df_ml = df.dropna(subset=X_features + [target])\n",
    "X = df_ml[X_features]\n",
    "y = df_ml[target]\n",
    "\n",
    "# 4. TRAIN–TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 5. TRAIN RANDOM FOREST REGRESSOR\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    min_samples_leaf=2\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. EVALUATION\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"--- Regression Performance ---\")\n",
    "print(f\"MAE  (Mean Absolute Error): {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"RMSE (Root Mean Sq Error): {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
    "print(f\"R²   (Explained Variance): {r2_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "# 7. FEATURE IMPORTANCE\n",
    "feat_df = pd.DataFrame({\n",
    "    'Feature': X_features,\n",
    "    'Importance': model.feature_importances_,\n",
    "    'Category': (['Rain'] * 2) + (['Buildings'] * 3) + (['Topology'] * 2)\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# 8. VISUALISATION\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=feat_df,\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    hue='Category',\n",
    "    dodge=False\n",
    ")\n",
    "plt.title('Drivers of Flood Frequency in Bengaluru (Regression)')\n",
    "plt.xlabel('Relative Contribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. CATEGORY-LEVEL IMPORTANCE\n",
    "cat_importance = feat_df.groupby('Category')['Importance'].sum().sort_values(ascending=False)\n",
    "print(\"\\n--- Importance by Category ---\")\n",
    "print(cat_importance)\n",
    "\n",
    "# 10. SAVE PREDICTIONS\n",
    "results = df_ml.loc[X_test.index, ['name', target]].copy()\n",
    "results['predicted_flood_freq_pct'] = y_pred\n",
    "results.to_csv('final_flood_frequency_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

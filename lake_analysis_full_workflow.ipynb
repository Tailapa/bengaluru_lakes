{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e1d65c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1475053a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized!\n"
     ]
    }
   ],
   "source": [
    "# ee.Authenticate()\n",
    "project_id = 'bengaluru-lakes-485612'\n",
    "\n",
    "try:\n",
    "    ee.Initialize(project=project_id)\n",
    "    print(\"Successfully initialized!\")\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a37488f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for lakes in Bengaluru, Karnataka, India...\n",
      "Found 202 lakes with names.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>element</th>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">relation</th>\n",
       "      <th>1332093</th>\n",
       "      <td>NCBS Pond</td>\n",
       "      <td>POLYGON ((77.5791 13.07125, 77.57909 13.07121,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853330</th>\n",
       "      <td>Vengayyana Lake</td>\n",
       "      <td>POLYGON ((77.70218 13.01708, 77.70235 13.017, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857615</th>\n",
       "      <td>Halasuru lake</td>\n",
       "      <td>POLYGON ((77.62261 12.98202, 77.6227 12.98193,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310400</th>\n",
       "      <td>Chelekere</td>\n",
       "      <td>POLYGON ((77.64527 13.02519, 77.64512 13.02543...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310417</th>\n",
       "      <td>Madiwala Lake</td>\n",
       "      <td>MULTIPOLYGON (((77.61159 12.90261, 77.61165 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name  \\\n",
       "element  id                         \n",
       "relation 1332093        NCBS Pond   \n",
       "         1853330  Vengayyana Lake   \n",
       "         1857615    Halasuru lake   \n",
       "         2310400        Chelekere   \n",
       "         2310417    Madiwala Lake   \n",
       "\n",
       "                                                           geometry  \n",
       "element  id                                                          \n",
       "relation 1332093  POLYGON ((77.5791 13.07125, 77.57909 13.07121,...  \n",
       "         1853330  POLYGON ((77.70218 13.01708, 77.70235 13.017, ...  \n",
       "         1857615  POLYGON ((77.62261 12.98202, 77.6227 12.98193,...  \n",
       "         2310400  POLYGON ((77.64527 13.02519, 77.64512 13.02543...  \n",
       "         2310417  MULTIPOLYGON (((77.61159 12.90261, 77.61165 12...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Bengaluru boundary\n",
    "place_name = 'Bengaluru, Karnataka, India'\n",
    "\n",
    "# Query lakes using tags: natural=water and water=lake\n",
    "tags = {'natural': 'water', 'water': 'lake'}\n",
    "\n",
    "print(f'Searching for lakes in {place_name}...')\n",
    "try:\n",
    "    # Use osmnx to fetch features\n",
    "    lakes_gdf = ox.features_from_place(place_name, tags)\n",
    "    \n",
    "    # Filter for polygons and multipolygons\n",
    "    lakes_gdf = lakes_gdf[lakes_gdf.geometry.type.isin(['Polygon', 'MultiPolygon'])]\n",
    "    \n",
    "    # Keep only relevant columns and drop rows without names\n",
    "    lakes_gdf = lakes_gdf[['name', 'geometry']].dropna(subset=['name'])\n",
    "    \n",
    "    print(f'Found {len(lakes_gdf)} lakes with names.')\n",
    "    display(lakes_gdf.head())\n",
    "except Exception as e:\n",
    "    print(f'Error retrieving lakes: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0695738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_lake_area_fast(lake_row, start_year=2020, end_year=2025):\n",
    "    lake_name = lake_row['name']\n",
    "    print(f\"processing {lake_name}...\")\n",
    "    lon, lat = lake_row.geometry.centroid.x, lake_row.geometry.centroid.y\n",
    "    \n",
    "    # Pre-process geometries\n",
    "    lake_geom_ee = geemap.gdf_to_ee(gpd.GeoDataFrame([lake_row], crs=lakes_gdf.crs))\n",
    "    strict_boundary = lake_geom_ee.geometry()\n",
    "    buffered_boundary = strict_boundary.buffer(50)\n",
    "    \n",
    "    # CALCULATE POTENTIAL CAPACITY (Static geometric area of the polygon)\n",
    "    # We use a pixelArea of 1 multiplied by the reducer to get the total polygon size in m2\n",
    "    potential_area_m2 = ee.Image.pixelArea().reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=strict_boundary,\n",
    "        scale=10, # Sentinel resolution for precision\n",
    "        maxPixels=1e9\n",
    "    ).get('area')\n",
    "    \n",
    "    # Convert to ha on the server side\n",
    "    potential_ha = ee.Number(potential_area_m2).divide(10000)\n",
    "    \n",
    "    years = ee.List.sequence(start_year, end_year)\n",
    "\n",
    "    def calculate_annual_stats(year):\n",
    "        year = ee.Number(year)\n",
    "        \n",
    "        # Select collection based on year\n",
    "        s2_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "                  .filterBounds(buffered_boundary)\n",
    "                  .filter(ee.Filter.calendarRange(year, year, 'year'))\n",
    "                  .filter(ee.Filter.calendarRange(11, 2, 'month')) \n",
    "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
    "                  .select(['B3', 'B11', 'B8', 'B4'], ['Green', 'SWIR1', 'NIR', 'Red']))\n",
    "        \n",
    "        l8_col = (ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n",
    "                  .filterBounds(buffered_boundary)\n",
    "                  .filter(ee.Filter.calendarRange(year, year, 'year'))\n",
    "                  .filter(ee.Filter.calendarRange(11, 2, 'month'))\n",
    "                  .filter(ee.Filter.lt('CLOUD_COVER', 20))\n",
    "                  .select(['SR_B3', 'SR_B6', 'SR_B5', 'SR_B4'], ['Green', 'SWIR1', 'NIR', 'Red']))\n",
    "        \n",
    "        collection = ee.ImageCollection(ee.Algorithms.If(year.gte(2017), s2_col, l8_col))\n",
    "        scale = ee.Number(ee.Algorithms.If(year.gte(2017), 10, 30))\n",
    "        \n",
    "        image = collection.median()\n",
    "        \n",
    "        # Indices\n",
    "        mndwi = image.normalizedDifference(['Green', 'SWIR1']).rename('water')\n",
    "        ndvi = image.normalizedDifference(['NIR', 'Red']).rename('weed')\n",
    "        \n",
    "        # Masks\n",
    "        water_mask = mndwi.gt(0)\n",
    "        weed_mask = ndvi.gt(0.4).And(mndwi.gt(-0.5))\n",
    "        combined_mask = water_mask.Or(weed_mask).rename('total')\n",
    "        \n",
    "        # Area image\n",
    "        area_img = ee.Image.cat([water_mask, weed_mask, combined_mask]).multiply(ee.Image.pixelArea())\n",
    "        \n",
    "        # REDUCTION 1: Static Boundary\n",
    "        stats_static = area_img.reduceRegion(\n",
    "            reducer=ee.Reducer.sum(),\n",
    "            geometry=strict_boundary,\n",
    "            scale=scale,\n",
    "            maxPixels=1e9\n",
    "        )\n",
    "        \n",
    "        # REDUCTION 2: Buffered Boundary (50m)\n",
    "        stats_buffer = area_img.reduceRegion(\n",
    "            reducer=ee.Reducer.sum(),\n",
    "            geometry=buffered_boundary,\n",
    "            scale=scale,\n",
    "            maxPixels=1e9\n",
    "        )\n",
    "\n",
    "        # Helper to get numbers safely\n",
    "        def get_ha(stats, key):\n",
    "            return ee.Number(stats.get(key, 0)).divide(10000)\n",
    "\n",
    "        return ee.Feature(None, {\n",
    "            'name': lake_name,\n",
    "            'year': year,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'potential_ha': potential_ha, # This stays constant for all years of a lake\n",
    "            # Static Results\n",
    "            'static_water_ha': get_ha(stats_static, 'water'),\n",
    "            'static_weed_ha': get_ha(stats_static, 'weed'),\n",
    "            'static_total_ha': get_ha(stats_static, 'total'),\n",
    "            # Buffered Results\n",
    "            'buffer_water_ha': get_ha(stats_buffer, 'water'),\n",
    "            'buffer_weed_ha': get_ha(stats_buffer, 'weed'),\n",
    "            'buffer_total_ha': get_ha(stats_buffer, 'total')\n",
    "        })\n",
    "\n",
    "    annual_features = ee.FeatureCollection(years.map(calculate_annual_stats))\n",
    "    return pd.DataFrame([f['properties'] for f in annual_features.getInfo()['features']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89f9185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing for 202 lakes...\n",
      "processing NCBS Pond...\n",
      "processing Vengayyana Lake...\n",
      "processing Halasuru lake...\n",
      "processing Chelekere...\n",
      "processing Madiwala Lake...\n",
      "processing Iblur Lake...\n",
      "processing Benniganahalli Lake...\n",
      "processing Puttenhalli Lake...\n",
      "processing Mathikere Lake...\n",
      "processing Anchepalya Lake...\n",
      "processing Sankey Tank...\n",
      "processing Rachenahalli Lake...\n",
      "processing Chinnappanahalli Lake...\n",
      "processing Herohalli Kere...\n",
      "processing Kaikondrahalli Lake...\n",
      "processing Lal Bahadur Shastri Nagar Lake...\n",
      "processing Agara Lake...\n",
      "processing Hebbal Lake...\n",
      "processing Sarakki Kere...\n",
      "processing Jakkur Lake...\n",
      "processing Yediyur Lake...\n",
      "processing Seegehalli Lake...\n",
      "processing Yelahanka Lake...\n",
      "processing Hemmigepura Kere...\n",
      "processing Thubarahalli Lake...\n",
      "processing Agrahara Lake...\n",
      "processing Puttenahalli Lake...\n",
      "processing Attur Lake...\n",
      "processing Allalasandra Lake...\n",
      "processing Kodigehalli Lake...\n",
      "processing Narsipura Lake...\n",
      "processing Dasarahalli Tank...\n",
      "processing Subramanyapura Lake...\n",
      "processing Panathuru Kere...\n",
      "processing Deverabisanahalli Lake...\n",
      "processing Doddakannelli Lake...\n",
      "processing Byrasandra Lake...\n",
      "processing Nayandahalli Lake...\n",
      "processing Kalkere Lake...\n",
      "processing Horamavu Agara Lake...\n",
      "processing Goshala Lake...\n",
      "processing Mahadevapura Lake...\n",
      "processing Yelachenahalli Kere...\n",
      "processing Haraluru Lake...\n",
      "processing ITI Layout Lake...\n",
      "processing Panathur Chikka Kere...\n",
      "processing Munekolala Lake...\n",
      "processing Seetharampalya Lake...\n",
      "processing Doddathogur Lake...\n",
      "processing Kaggadasapura Lake...\n",
      "processing Bamboo Island Pond...\n",
      "processing Doddabommasandra Kere...\n",
      "processing Varthur Lake...\n",
      "processing Ramapura Kere...\n",
      "processing Thalaghattapura Lake...\n",
      "processing Begur Lake...\n",
      "processing Arekere lake...\n",
      "processing Kalena Agrahara lake...\n",
      "processing Bellandur Lake...\n",
      "processing Hulimavu Lake...\n",
      "processing Gottigere Tank...\n",
      "processing Chikka Bellanduru Lake...\n",
      "processing Carmelaram Lake...\n",
      "processing Ambalipura Upper Lake...\n",
      "processing Yellamallappa Chetty Lake...\n",
      "processing Byrasandra Lake...\n",
      "processing Nagawara Lake...\n",
      "processing Hoodi Lake...\n",
      "processing Chunchagatta Lake...\n",
      "processing Doddanekundi Lake...\n",
      "processing ISRO Layout Lake...\n",
      "processing Dore Kere...\n",
      "processing Mallathahalli Lake...\n",
      "processing Kempambudhi Kere...\n",
      "processing Gowdana Kere...\n",
      "processing Hosakerahalli Lake...\n",
      "processing Allalasandra Lake...\n",
      "processing Gantiganahalli Lake...\n",
      "processing Veerasagara Lake...\n",
      "processing Singapura Kere...\n",
      "processing Abbigere Lake...\n",
      "processing Kogilu Lake...\n",
      "processing BEL Lake...\n",
      "processing Amruthalli Lake...\n",
      "processing Thirumenahalli Lake...\n",
      "processing Chokkanahalli Lake...\n",
      "processing Kattigenahalli Lake...\n",
      "processing Kowdenahalli Lake...\n",
      "processing Horamavu Lake...\n",
      "processing Devasandra lake...\n",
      "processing Hosakere...\n",
      "processing Dubasi Palya Kere...\n",
      "processing Sheelavathana Kere...\n",
      "processing Lotus Pond...\n",
      "processing Sampangi Lake...\n",
      "processing Kasavanahalli Lake...\n",
      "processing Garudacharpalya Lake...\n",
      "processing Haralur Lake...\n",
      "processing Basapura Lake...\n",
      "processing Basapura Lake...\n",
      "processing Deepanjali Nagara Kere...\n",
      "processing Vasantapura Temple Kalyani...\n",
      "processing Krishna Nagara Lake...\n",
      "processing Avalahalli Lake...\n",
      "processing Chudahalli Lake...\n",
      "processing Doddakalasandra Lake...\n",
      "processing Vasanthapura Lake...\n",
      "processing Chikka Beguru Lake...\n",
      "processing Kundalahalli Lake...\n",
      "processing Pattandur Agrahara Kunte...\n",
      "processing Akshaynagar Lake...\n",
      "processing Nallurahalli Lake...\n",
      "processing Halagevader Halli Lake...\n",
      "processing Hosakere...\n",
      "processing Singasandra Lake...\n",
      "processing Hoodi Lake...\n",
      "processing Kammagondanahalli Lake...\n",
      "processing B Narayanapura Lake...\n",
      "processing Basavanapura Kere...\n",
      "processing Subramanya Swamy Temple Kalyani...\n",
      "processing Andrahalli Lake...\n",
      "processing Ullal Lake...\n",
      "processing Gunjuru Lake...\n",
      "processing Brigade Gateway Lake...\n",
      "processing Uttarahalli Lake...\n",
      "processing Sunkalpalya Lake...\n",
      "processing Mylasandra Tank...\n",
      "processing Konasandra Lake...\n",
      "processing Lalbagh Tank...\n",
      "processing Byrasandra Melina Kere...\n",
      "processing Kacharakanahalli Lake...\n",
      "processing Palace Grounds Lake...\n",
      "processing Kadiranpalaya Kere...\n",
      "processing Mallasandra Lake...\n",
      "processing Doddkammanahalli Lake...\n",
      "processing Ambalipura Lower Lake...\n",
      "processing Yelenahalli Lake...\n",
      "processing Somasundarapalya Lake...\n",
      "processing Bheemanakatte Lake...\n",
      "processing Kenchanahalli Lake...\n",
      "processing Immersion Tank...\n",
      "processing K100...\n",
      "processing Saul Kere...\n",
      "processing Parappana Agrahara Lake...\n",
      "processing Kengeri Lake...\n",
      "processing Konanakunte Lake...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing Singapura Kere...\n",
      "processing Kalyani...\n",
      "processing Small Pool...\n",
      "processing Kuppareddi Kere...\n",
      "processing Vidyaranyapura Kere...\n",
      "processing Centenary Pond...\n",
      "processing Bagalagunte Lake...\n",
      "processing Narayanapura Kere...\n",
      "processing Ground...\n",
      "processing Halcyon Swimming Pool...\n",
      "processing ISKCON Temple Pond...\n",
      "processing Water Tank...\n",
      "processing Doddabidirakallu Kere...\n",
      "processing Karagada Kunte...\n",
      "processing Vijinapura Lake...\n",
      "processing Oracle Tech Hub Entrance Fountain...\n",
      "processing Chikkalasandra lake...\n",
      "processing Shivapura Old Lake...\n",
      "processing Nagarbhavi Lake...\n",
      "processing Nagaraesh Nagenahalli Lake...\n",
      "processing Bhoganahalli Lake...\n",
      "processing Pattandur Agrahara Lake...\n",
      "processing Palanahalli Lake...\n",
      "processing Subedharana Lake...\n",
      "processing Nagarbhavi Thorai...\n",
      "processing Vrishabhavati...\n",
      "processing Nagarbhavi Thorai...\n",
      "processing Nagarbhavi Thorai...\n",
      "processing Subrayana Lake...\n",
      "processing Gunjuru Palya Lake...\n",
      "processing K100...\n",
      "processing Jogi Kere...\n",
      "processing Gubbalala Kere...\n",
      "processing Sampigehalli Lake...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing Thippasandra Lake...\n",
      "processing Chikkabettahalli Lake...\n",
      "processing Basavanapura Pond...\n",
      "processing Chandrasekhar Layout Lake...\n",
      "processing Kaveri Water Tank...\n",
      "processing Kalyani...\n",
      "processing Chokkanahalli Lake...\n",
      "processing Lakshmipura Lake...\n",
      "processing Krishna Nagara Lake...\n",
      "processing Lingadeeranahalli Lake...\n",
      "processing Srigandakaval Lake...\n",
      "processing B Channasandra Lake...\n",
      "processing Idle Lake...\n",
      "------------------------------\n",
      "Processing Complete!\n",
      "Total rows generated: 1212\n",
      "File saved to: data/bengaluru_lakes_master_2020_2025.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Ensure the output directory exists\n",
    "output_dir = 'data'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 2. List to store the results of each lake\n",
    "all_lakes_results = []\n",
    "\n",
    "print(f\"Starting batch processing for {len(lakes_gdf)} lakes...\")\n",
    "\n",
    "# 3. Loop through all 202 lakes in lakes_gdf\n",
    "for idx in range(len(lakes_gdf)):\n",
    "    lake_row = lakes_gdf.iloc[idx]\n",
    "    \n",
    "    try:\n",
    "        # Call the fast server-side function\n",
    "        # Using 2020-2025 as the default range\n",
    "        df_lake = get_lake_area_fast(lake_row, start_year=2020, end_year=2025)\n",
    "        \n",
    "        # Add to our collection\n",
    "        all_lakes_results.append(df_lake)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {lake_row['name']} (Index {idx}): {e}\")\n",
    "\n",
    "# 4. Concatenate all individual DataFrames into one master table\n",
    "master_df = pd.concat(all_lakes_results, ignore_index=True)\n",
    "\n",
    "# 5. Save the final result to the data folder\n",
    "csv_filename = os.path.join(output_dir, 'bengaluru_lakes_master_2020_2025.csv')\n",
    "master_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Processing Complete!\")\n",
    "print(f\"Total rows generated: {len(master_df)}\")\n",
    "print(f\"File saved to: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad43074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_land_cover_stats(lake_row, start_year=2020, end_year=2025):\n",
    "    lake_name = lake_row['name']\n",
    "    print(f\"processing {lake_name}...\")\n",
    "    \n",
    "    # 1. Setup Geometries\n",
    "    lake_gdf_single = gpd.GeoDataFrame([lake_row], crs=lakes_gdf.crs)\n",
    "    lake_geom_ee = geemap.gdf_to_ee(lake_gdf_single)\n",
    "    strict_boundary = lake_geom_ee.geometry()\n",
    "    \n",
    "    # Create the Donut Zones\n",
    "    outer_buffer = strict_boundary.buffer(50)\n",
    "    inner_buffer = strict_boundary.buffer(-50)\n",
    "    \n",
    "    # Zone A: 50m Inside the boundary (The Shoreline/Riparian zone)\n",
    "    inside_50m_zone = strict_boundary.difference(inner_buffer)\n",
    "    \n",
    "    # Zone B: 50m Outside the boundary (The Neighborhood/Encroachment zone)\n",
    "    outside_50m_zone = outer_buffer.difference(strict_boundary)\n",
    "    \n",
    "    years = ee.List.sequence(start_year, end_year)\n",
    "\n",
    "    def calculate_annual_land_cover(year):\n",
    "        year = ee.Number(year)\n",
    "        \n",
    "        # We use Sentinel-2 for better resolution (10m) on buildings\n",
    "        collection = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "                      .filterBounds(outer_buffer)\n",
    "                      .filter(ee.Filter.calendarRange(year, year, 'year'))\n",
    "                      .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
    "                      .median())\n",
    "        \n",
    "        # NDVI for Green Cover (NIR - Red) / (NIR + Red)\n",
    "        ndvi = collection.normalizedDifference(['B8', 'B4']).rename('ndvi')\n",
    "        \n",
    "        # NDBI for Buildings (SWIR - NIR) / (SWIR + NIR)\n",
    "        ndbi = collection.normalizedDifference(['B11', 'B8']).rename('ndbi')\n",
    "        \n",
    "        # Classification Thresholds\n",
    "        green_mask = ndvi.gt(0.4).rename('green')\n",
    "        building_mask = ndbi.gt(0.0).And(ndvi.lt(0.2)).rename('buildings')\n",
    "        \n",
    "        stats_img = ee.Image.cat([green_mask, building_mask]).multiply(ee.Image.pixelArea())\n",
    "        \n",
    "        # Helper to reduce by zone\n",
    "        def get_stats(zone_geom):\n",
    "            return stats_img.reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=zone_geom,\n",
    "                scale=10,\n",
    "                maxPixels=1e9\n",
    "            )\n",
    "\n",
    "        res_inside = get_stats(inside_50m_zone)\n",
    "        res_outside = get_stats(outside_50m_zone)\n",
    "\n",
    "        return ee.Feature(None, {\n",
    "            'name': lake_name, 'year': year,\n",
    "            'in_green_ha': ee.Number(res_inside.get('green', 0)).divide(10000),\n",
    "            'in_build_ha': ee.Number(res_inside.get('buildings', 0)).divide(10000),\n",
    "            'out_green_ha': ee.Number(res_outside.get('green', 0)).divide(10000),\n",
    "            'out_build_ha': ee.Number(res_outside.get('buildings', 0)).divide(10000)\n",
    "        })\n",
    "\n",
    "    annual_fc = ee.FeatureCollection(years.map(calculate_annual_land_cover))\n",
    "    return pd.DataFrame([f['properties'] for f in annual_fc.getInfo()['features']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9fd472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing NCBS Pond...\n",
      "processing Vengayyana Lake...\n",
      "processing Halasuru lake...\n",
      "processing Chelekere...\n",
      "processing Madiwala Lake...\n",
      "processing Iblur Lake...\n",
      "processing Benniganahalli Lake...\n",
      "processing Puttenhalli Lake...\n",
      "processing Mathikere Lake...\n",
      "processing Anchepalya Lake...\n",
      "processing Sankey Tank...\n",
      "processing Rachenahalli Lake...\n",
      "processing Chinnappanahalli Lake...\n",
      "processing Herohalli Kere...\n",
      "processing Kaikondrahalli Lake...\n",
      "processing Lal Bahadur Shastri Nagar Lake...\n",
      "processing Agara Lake...\n",
      "processing Hebbal Lake...\n",
      "processing Sarakki Kere...\n",
      "processing Jakkur Lake...\n",
      "processing Yediyur Lake...\n",
      "processing Seegehalli Lake...\n",
      "processing Yelahanka Lake...\n",
      "processing Hemmigepura Kere...\n",
      "processing Thubarahalli Lake...\n",
      "processing Agrahara Lake...\n",
      "processing Puttenahalli Lake...\n",
      "processing Attur Lake...\n",
      "processing Allalasandra Lake...\n",
      "processing Kodigehalli Lake...\n",
      "processing Narsipura Lake...\n",
      "processing Dasarahalli Tank...\n",
      "processing Subramanyapura Lake...\n",
      "processing Panathuru Kere...\n",
      "processing Deverabisanahalli Lake...\n",
      "processing Doddakannelli Lake...\n",
      "processing Byrasandra Lake...\n",
      "processing Nayandahalli Lake...\n",
      "processing Kalkere Lake...\n",
      "processing Horamavu Agara Lake...\n",
      "processing Goshala Lake...\n",
      "processing Mahadevapura Lake...\n",
      "processing Yelachenahalli Kere...\n",
      "processing Haraluru Lake...\n",
      "processing ITI Layout Lake...\n",
      "processing Panathur Chikka Kere...\n",
      "processing Munekolala Lake...\n",
      "processing Seetharampalya Lake...\n",
      "processing Doddathogur Lake...\n",
      "processing Kaggadasapura Lake...\n",
      "processing Bamboo Island Pond...\n",
      "processing Doddabommasandra Kere...\n",
      "processing Varthur Lake...\n",
      "processing Ramapura Kere...\n",
      "processing Thalaghattapura Lake...\n",
      "processing Begur Lake...\n",
      "processing Arekere lake...\n",
      "processing Kalena Agrahara lake...\n",
      "processing Bellandur Lake...\n",
      "processing Hulimavu Lake...\n",
      "processing Gottigere Tank...\n",
      "processing Chikka Bellanduru Lake...\n",
      "processing Carmelaram Lake...\n",
      "processing Ambalipura Upper Lake...\n",
      "processing Yellamallappa Chetty Lake...\n",
      "processing Byrasandra Lake...\n",
      "processing Nagawara Lake...\n",
      "processing Hoodi Lake...\n",
      "processing Chunchagatta Lake...\n",
      "processing Doddanekundi Lake...\n",
      "processing ISRO Layout Lake...\n",
      "processing Dore Kere...\n",
      "processing Mallathahalli Lake...\n",
      "processing Kempambudhi Kere...\n",
      "processing Gowdana Kere...\n",
      "processing Hosakerahalli Lake...\n",
      "processing Allalasandra Lake...\n",
      "processing Gantiganahalli Lake...\n",
      "processing Veerasagara Lake...\n",
      "processing Singapura Kere...\n",
      "processing Abbigere Lake...\n",
      "processing Kogilu Lake...\n",
      "processing BEL Lake...\n",
      "processing Amruthalli Lake...\n",
      "processing Thirumenahalli Lake...\n",
      "processing Chokkanahalli Lake...\n",
      "processing Kattigenahalli Lake...\n",
      "processing Kowdenahalli Lake...\n",
      "processing Horamavu Lake...\n",
      "processing Devasandra lake...\n",
      "processing Hosakere...\n",
      "processing Dubasi Palya Kere...\n",
      "processing Sheelavathana Kere...\n",
      "processing Lotus Pond...\n",
      "processing Sampangi Lake...\n",
      "processing Kasavanahalli Lake...\n",
      "processing Garudacharpalya Lake...\n",
      "processing Haralur Lake...\n",
      "processing Basapura Lake...\n",
      "processing Basapura Lake...\n",
      "processing Deepanjali Nagara Kere...\n",
      "processing Vasantapura Temple Kalyani...\n",
      "processing Krishna Nagara Lake...\n",
      "processing Avalahalli Lake...\n",
      "processing Chudahalli Lake...\n",
      "processing Doddakalasandra Lake...\n",
      "processing Vasanthapura Lake...\n",
      "processing Chikka Beguru Lake...\n",
      "processing Kundalahalli Lake...\n",
      "processing Pattandur Agrahara Kunte...\n",
      "processing Akshaynagar Lake...\n",
      "processing Nallurahalli Lake...\n",
      "processing Halagevader Halli Lake...\n",
      "processing Hosakere...\n",
      "processing Singasandra Lake...\n",
      "processing Hoodi Lake...\n",
      "processing Kammagondanahalli Lake...\n",
      "processing B Narayanapura Lake...\n",
      "processing Basavanapura Kere...\n",
      "processing Subramanya Swamy Temple Kalyani...\n",
      "processing Andrahalli Lake...\n",
      "processing Ullal Lake...\n",
      "processing Gunjuru Lake...\n",
      "processing Brigade Gateway Lake...\n",
      "processing Uttarahalli Lake...\n",
      "processing Sunkalpalya Lake...\n",
      "processing Mylasandra Tank...\n",
      "processing Konasandra Lake...\n",
      "processing Lalbagh Tank...\n",
      "processing Byrasandra Melina Kere...\n",
      "processing Kacharakanahalli Lake...\n",
      "processing Palace Grounds Lake...\n",
      "processing Kadiranpalaya Kere...\n",
      "processing Mallasandra Lake...\n",
      "processing Doddkammanahalli Lake...\n",
      "processing Ambalipura Lower Lake...\n",
      "processing Yelenahalli Lake...\n",
      "processing Somasundarapalya Lake...\n",
      "processing Bheemanakatte Lake...\n",
      "processing Kenchanahalli Lake...\n",
      "processing Immersion Tank...\n",
      "processing K100...\n",
      "processing Saul Kere...\n",
      "processing Parappana Agrahara Lake...\n",
      "processing Kengeri Lake...\n",
      "processing Konanakunte Lake...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing Singapura Kere...\n",
      "processing Kalyani...\n",
      "processing Small Pool...\n",
      "processing Kuppareddi Kere...\n",
      "processing Vidyaranyapura Kere...\n",
      "processing Centenary Pond...\n",
      "processing Bagalagunte Lake...\n",
      "processing Narayanapura Kere...\n",
      "processing Ground...\n",
      "processing Halcyon Swimming Pool...\n",
      "processing ISKCON Temple Pond...\n",
      "processing Water Tank...\n",
      "processing Doddabidirakallu Kere...\n",
      "processing Karagada Kunte...\n",
      "processing Vijinapura Lake...\n",
      "processing Oracle Tech Hub Entrance Fountain...\n",
      "processing Chikkalasandra lake...\n",
      "processing Shivapura Old Lake...\n",
      "processing Nagarbhavi Lake...\n",
      "processing Nagaraesh Nagenahalli Lake...\n",
      "processing Bhoganahalli Lake...\n",
      "processing Pattandur Agrahara Lake...\n",
      "processing Palanahalli Lake...\n",
      "processing Subedharana Lake...\n",
      "processing Nagarbhavi Thorai...\n",
      "processing Vrishabhavati...\n",
      "processing Nagarbhavi Thorai...\n",
      "processing Nagarbhavi Thorai...\n",
      "processing Subrayana Lake...\n",
      "processing Gunjuru Palya Lake...\n",
      "processing K100...\n",
      "processing Jogi Kere...\n",
      "processing Gubbalala Kere...\n",
      "processing Sampigehalli Lake...\n",
      "processing K100...\n",
      "processing K100...\n",
      "processing Thippasandra Lake...\n",
      "processing Chikkabettahalli Lake...\n",
      "processing Basavanapura Pond...\n",
      "processing Chandrasekhar Layout Lake...\n",
      "processing Kaveri Water Tank...\n",
      "processing Kalyani...\n",
      "processing Chokkanahalli Lake...\n",
      "processing Lakshmipura Lake...\n",
      "processing Krishna Nagara Lake...\n",
      "processing Lingadeeranahalli Lake...\n",
      "processing Srigandakaval Lake...\n",
      "processing B Channasandra Lake...\n",
      "processing Idle Lake...\n"
     ]
    }
   ],
   "source": [
    "all_land_results = []\n",
    "\n",
    "for idx in range(len(lakes_gdf)):\n",
    "    lake_row = lakes_gdf.iloc[idx]\n",
    "    try:\n",
    "        df_land = get_land_cover_stats(lake_row)\n",
    "        all_land_results.append(df_land)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {lake_row['name']}: {e}\")\n",
    "\n",
    "# Save the final Land Cover dataset\n",
    "if all_land_results:\n",
    "    master_land_df = pd.concat(all_land_results, ignore_index=True)\n",
    "    master_land_df.to_csv('data/bengaluru_lakes_land_cover.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c627749e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved. Remaining rows: 1086\n"
     ]
    }
   ],
   "source": [
    "# Load the hydrological master data\n",
    "df_master = pd.read_csv('data/bengaluru_lakes_master_2020_2025.csv')\n",
    "\n",
    "# Load the land cover (buildings/greenery) data\n",
    "df_land = pd.read_csv('data/bengaluru_lakes_land_cover.csv')\n",
    "\n",
    "# Merge on common keys: 'name' and 'year'\n",
    "df_combined = pd.merge(df_master, df_land, on=['name', 'year'], how='inner')\n",
    "\n",
    "# Save the final consolidated dataset\n",
    "df_combined.to_csv('data/bengaluru_lakes_combined_data_2020_2025.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('data/bengaluru_lakes_combined_data_2020_2025.csv')\n",
    "\n",
    "# Remove duplicates based on name and year, keeping the first entry found\n",
    "df_cleaned = df.drop_duplicates(subset=['name', 'year'], keep='first')\n",
    "\n",
    "# Save the cleaned version\n",
    "df_cleaned.to_csv('data/bengaluru_lakes.csv', index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved. Remaining rows: {len(df_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e55ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lon",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "potential_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "static_total_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "static_water_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "static_weed_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "buffer_total_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "buffer_water_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "buffer_weed_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "in_build_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "out_build_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "in_green_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "out_green_ha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "encroachment_pct",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "159cd4fc-e120-4348-8725-d19fd4b20919",
       "rows": [
        [
         "32",
         "Chikkabettahalli Lake",
         "13.091796866719235",
         "77.55472093578341",
         "1.86306800114052",
         "0.5215245058431189",
         "0.4959411987603879",
         "0.02558330708273095",
         "0.9120383390533847",
         "0.6163906691233317",
         "0.2956476699300529",
         "1.5233158345964217",
         "2.523840613023846",
         "0.025975631777196116",
         "0.2901423846096463",
         "81.76383436696291"
        ],
        [
         "119",
         "Panathur Chikka Kere",
         "12.931470394744794",
         "77.70863691702442",
         "0.9048249040252087",
         "0.4739722989754272",
         "0.4691411493558198",
         "0.0063887690031762005",
         "0.8527020245381274",
         "0.48599637158711745",
         "0.36832025840958743",
         "0.734791194065069",
         "0.9898987937608107",
         "0.0029569435608308835",
         "0.5095063952013402",
         "81.20810897183236"
        ],
        [
         "35",
         "Chokkanahalli Lake",
         "13.08461334299774",
         "77.62922109689285",
         "2.022320230141434",
         "0.45589922443364966",
         "0.4146471392149083",
         "0.04125208521874118",
         "1.0284739288245308",
         "0.4179313624064127",
         "0.610542566418118",
         "1.5262763552008733",
         "0.954972362733729",
         "0.0543256796713436",
         "1.1465483148063709",
         "75.47154661525248"
        ],
        [
         "120",
         "Panathuru Kere",
         "12.931615520659848",
         "77.70685455116339",
         "4.149419714241775",
         "2.738648957746418",
         "2.69067935862323",
         "0.052477823999442265",
         "4.448213682342231",
         "2.7432141474405927",
         "1.7114579465634214",
         "2.8383098958418103",
         "1.195444451721291",
         "0.03446386508517793",
         "2.0423954177188253",
         "68.40257412621021"
        ],
        [
         "95",
         "Krishna Nagara Lake",
         "12.874035219254122",
         "77.57974859667902",
         "4.421779224144433",
         "2.770499086692162",
         "2.670206791858549",
         "0.10029229483361334",
         "4.383426257412979",
         "2.6937663983662925",
         "1.6896598590466863",
         "2.7994158178476574",
         "0.9411869539053924",
         "0.1347068356617447",
         "3.034443859465855",
         "63.30971484423931"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>potential_ha</th>\n",
       "      <th>static_total_ha</th>\n",
       "      <th>static_water_ha</th>\n",
       "      <th>static_weed_ha</th>\n",
       "      <th>buffer_total_ha</th>\n",
       "      <th>buffer_water_ha</th>\n",
       "      <th>buffer_weed_ha</th>\n",
       "      <th>in_build_ha</th>\n",
       "      <th>out_build_ha</th>\n",
       "      <th>in_green_ha</th>\n",
       "      <th>out_green_ha</th>\n",
       "      <th>encroachment_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Chikkabettahalli Lake</td>\n",
       "      <td>13.091797</td>\n",
       "      <td>77.554721</td>\n",
       "      <td>1.863068</td>\n",
       "      <td>0.521525</td>\n",
       "      <td>0.495941</td>\n",
       "      <td>0.025583</td>\n",
       "      <td>0.912038</td>\n",
       "      <td>0.616391</td>\n",
       "      <td>0.295648</td>\n",
       "      <td>1.523316</td>\n",
       "      <td>2.523841</td>\n",
       "      <td>0.025976</td>\n",
       "      <td>0.290142</td>\n",
       "      <td>81.763834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Panathur Chikka Kere</td>\n",
       "      <td>12.931470</td>\n",
       "      <td>77.708637</td>\n",
       "      <td>0.904825</td>\n",
       "      <td>0.473972</td>\n",
       "      <td>0.469141</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.852702</td>\n",
       "      <td>0.485996</td>\n",
       "      <td>0.368320</td>\n",
       "      <td>0.734791</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.509506</td>\n",
       "      <td>81.208109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Chokkanahalli Lake</td>\n",
       "      <td>13.084613</td>\n",
       "      <td>77.629221</td>\n",
       "      <td>2.022320</td>\n",
       "      <td>0.455899</td>\n",
       "      <td>0.414647</td>\n",
       "      <td>0.041252</td>\n",
       "      <td>1.028474</td>\n",
       "      <td>0.417931</td>\n",
       "      <td>0.610543</td>\n",
       "      <td>1.526276</td>\n",
       "      <td>0.954972</td>\n",
       "      <td>0.054326</td>\n",
       "      <td>1.146548</td>\n",
       "      <td>75.471547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Panathuru Kere</td>\n",
       "      <td>12.931616</td>\n",
       "      <td>77.706855</td>\n",
       "      <td>4.149420</td>\n",
       "      <td>2.738649</td>\n",
       "      <td>2.690679</td>\n",
       "      <td>0.052478</td>\n",
       "      <td>4.448214</td>\n",
       "      <td>2.743214</td>\n",
       "      <td>1.711458</td>\n",
       "      <td>2.838310</td>\n",
       "      <td>1.195444</td>\n",
       "      <td>0.034464</td>\n",
       "      <td>2.042395</td>\n",
       "      <td>68.402574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Krishna Nagara Lake</td>\n",
       "      <td>12.874035</td>\n",
       "      <td>77.579749</td>\n",
       "      <td>4.421779</td>\n",
       "      <td>2.770499</td>\n",
       "      <td>2.670207</td>\n",
       "      <td>0.100292</td>\n",
       "      <td>4.383426</td>\n",
       "      <td>2.693766</td>\n",
       "      <td>1.689660</td>\n",
       "      <td>2.799416</td>\n",
       "      <td>0.941187</td>\n",
       "      <td>0.134707</td>\n",
       "      <td>3.034444</td>\n",
       "      <td>63.309715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name        lat        lon  potential_ha  \\\n",
       "32   Chikkabettahalli Lake  13.091797  77.554721      1.863068   \n",
       "119   Panathur Chikka Kere  12.931470  77.708637      0.904825   \n",
       "35      Chokkanahalli Lake  13.084613  77.629221      2.022320   \n",
       "120         Panathuru Kere  12.931616  77.706855      4.149420   \n",
       "95     Krishna Nagara Lake  12.874035  77.579749      4.421779   \n",
       "\n",
       "     static_total_ha  static_water_ha  static_weed_ha  buffer_total_ha  \\\n",
       "32          0.521525         0.495941        0.025583         0.912038   \n",
       "119         0.473972         0.469141        0.006389         0.852702   \n",
       "35          0.455899         0.414647        0.041252         1.028474   \n",
       "120         2.738649         2.690679        0.052478         4.448214   \n",
       "95          2.770499         2.670207        0.100292         4.383426   \n",
       "\n",
       "     buffer_water_ha  buffer_weed_ha  in_build_ha  out_build_ha  in_green_ha  \\\n",
       "32          0.616391        0.295648     1.523316      2.523841     0.025976   \n",
       "119         0.485996        0.368320     0.734791      0.989899     0.002957   \n",
       "35          0.417931        0.610543     1.526276      0.954972     0.054326   \n",
       "120         2.743214        1.711458     2.838310      1.195444     0.034464   \n",
       "95          2.693766        1.689660     2.799416      0.941187     0.134707   \n",
       "\n",
       "     out_green_ha  encroachment_pct  \n",
       "32       0.290142         81.763834  \n",
       "119      0.509506         81.208109  \n",
       "35       1.146548         75.471547  \n",
       "120      2.042395         68.402574  \n",
       "95       3.034444         63.309715  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/bengaluru_lakes.csv')\n",
    "df = df[['name', 'lat', 'lon', 'year', 'potential_ha', \n",
    "         'static_total_ha', 'static_water_ha', 'static_weed_ha',\n",
    "         'buffer_total_ha', 'buffer_water_ha', 'buffer_weed_ha',\n",
    "         'in_build_ha', 'out_build_ha', 'in_green_ha', 'out_green_ha']]\n",
    "df = df.sort_values(['potential_ha', 'year'], ascending=False)\n",
    "df = df[df['potential_ha'] > 0.5]\n",
    "\n",
    "df.to_csv('data/bengaluru_lakes_cleaned_gt_0.5ha.csv')\n",
    "\n",
    "df_mean = df.groupby(['name', 'lat', 'lon']).mean(numeric_only=True).reset_index()\n",
    "df_mean = df_mean.sort_values(by='in_build_ha', ascending=False)\n",
    "df_mean = df_mean.drop(columns=['year'])\n",
    "df_mean['encroachment_pct'] = (df_mean['in_build_ha'] / df_mean['potential_ha']) * 100\n",
    "df_mean = df_mean.sort_values(by='encroachment_pct', ascending=False)\n",
    "\n",
    "df_mean.to_csv('data/bengaluru_lakes_mean.csv')\n",
    "df_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f41b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "\n",
    "# 1. Setup\n",
    "#Initialize Project\n",
    "\n",
    "def map_lake_health(lake_name, start_year='2020', end_year='2025'):\n",
    "    # Fetch actual irregular boundary from OSM\n",
    "    try:\n",
    "        lake_feature = geemap.osm_to_ee(f\"{lake_name}, Bengaluru\")\n",
    "        lake_boundary = lake_feature.geometry()\n",
    "    except:\n",
    "        print(f\"Boundary for {lake_name} not found. Check spelling.\")\n",
    "        return None\n",
    "\n",
    "    # Load Sentinel-2 and clip to the JAGGED boundary\n",
    "    s2_img = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "              .filterBounds(lake_boundary)\n",
    "              .filterDate(f'{start_year}-01-01', f'{end_year}-12-31')\n",
    "              .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
    "              .median()\n",
    "              .clip(lake_boundary))\n",
    "\n",
    "    # Calculate Indices\n",
    "    mndwi = s2_img.normalizedDifference(['B3', 'B11']).rename('Water')\n",
    "    ndvi = s2_img.normalizedDifference(['B8', 'B4']).rename('Vegetation')\n",
    "    ndbi = s2_img.normalizedDifference(['B11', 'B8']).rename('Buildings')\n",
    "\n",
    "    # NDTI (Turbidity/Silt Index) \n",
    "    # High values = Muddy water / Silt deposits\n",
    "    ndti = s2_img.normalizedDifference(['B4', 'B3']).rename('Silt')\n",
    "\n",
    "    # Create a mask to remove water and vegetation\n",
    "    non_water_mask = mndwi.lt(0)  # Only areas that are NOT water\n",
    "    non_veg_mask = ndvi.lt(0.2)   # Only areas that are NOT healthy plants\n",
    "\n",
    "    # Calculate the refined Built-up Index\n",
    "    refined_encroachment = ndbi.updateMask(non_water_mask).updateMask(non_veg_mask)\n",
    "\n",
    "    # Visualize\n",
    "    Map = geemap.Map()\n",
    "    Map.add_basemap('SATELLITE')\n",
    "    Map.centerObject(lake_boundary, 17)\n",
    "    \n",
    "    # Add Layers\n",
    "    Map.addLayer(ndbi, {'min': -0.1, 'max': 0.3, 'palette': ['white', 'red']}, 'Encroachment (Red)')\n",
    "    Map.addLayer(refined_encroachment, {'min': 0, 'max': 0.4, 'palette': ['white', 'darkred']}, 'Refined Encroachment')\n",
    "    Map.addLayer(ndti, {'min': -0.1, 'max': 0.2, 'palette': ['white', 'brown']}, '4. Silt/Turbidity (Brown)')\n",
    "    Map.addLayer(ndvi, {'min': 0, 'max': 0.6, 'palette': ['white', 'green']}, 'Weeds (Green)')\n",
    "    Map.addLayer(mndwi, {'min': -0.5, 'max': 0.2, 'palette': ['white', 'blue']}, 'Water (Blue)')\n",
    "    Map.addLayer(lake_boundary, {'color': 'yellow'}, 'Legal Boundary')\n",
    "    \n",
    "    Map.add_layer_control()\n",
    "    return Map\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# Just change the name here to visualize any lake!\n",
    "my_lake_map = map_lake_health(\"Panathur Chikka Kere\")\n",
    "my_lake_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb4f37",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Slope and Elevation\n",
    "This script performs a **lake-specific topographic analysis** using **exact lake boundary geometries** (not circular proxies). It converts lake polygons stored in a CSV into **Earth Engine FeatureCollections**, then extracts **mean elevation and mean slope** within each lakes true spatial footprint.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Preparing Lake Attributes and Boundaries\n",
    "- Two CSV files are loaded:\n",
    "  - One containing **lake attributes** (names, metadata).\n",
    "  - Another containing **exact lake boundary geometries** stored as **WKT (Well-Known Text)**.\n",
    "- Duplicate lake names in the boundary file are removed to ensure a **one-to-one join**.\n",
    "- The attribute table and boundary table are **merged on lake name**, attaching polygon geometry to each lake record.\n",
    "\n",
    "**Key idea:** this step upgrades the analysis from *approximate circular buffers* to **true lake outlines**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Converting WKT Geometries into Earth Engine Features\n",
    "- Each lakes geometry is read from the WKT string using **Shapely**.\n",
    "- Two cases are handled explicitly:\n",
    "  - **Polygon**  a single contiguous lake boundary.\n",
    "  - **MultiPolygon**  lakes with multiple disconnected basins or islands.\n",
    "- Coordinates are extracted from Shapely objects and converted into:\n",
    "  - **ee.Geometry.Polygon** or\n",
    "  - **ee.Geometry.MultiPolygon**\n",
    "- Each geometry is wrapped as an **ee.Feature** with the lake name as metadata.\n",
    "- All features are combined into a single **ee.FeatureCollection**.\n",
    "\n",
    "**Key idea:** Earth Engine cannot read WKT directly, so this step bridges **local vector geometry**  **cloud-based geospatial analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Preparing Topographic Layers\n",
    "- **SRTM elevation data** (~30 m resolution) is loaded.\n",
    "- Two topographic variables are derived:\n",
    "  - **Elevation**  absolute height of the lake basin.\n",
    "  - **Slope**  steepness of terrain inside the lake footprint.\n",
    "- These layers are stacked into a single **multi-band image** for efficient processing.\n",
    "\n",
    "**Key idea:** elevation gives **hydrological position**, slope gives **geomorphic integrity**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Extracting Mean Topography within Exact Lake Boundaries\n",
    "- `reduceRegions` is used with the **lake polygon FeatureCollection**.\n",
    "- For each lake polygon:\n",
    "  - Mean **elevation** is computed.\n",
    "  - Mean **slope** is computed.\n",
    "- Extraction is done at **30 m scale**, matching the SRTM resolution.\n",
    "\n",
    "**Key idea:** statistics are computed **only inside the real lake boundaries**, not across buffers or surrounding land.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5: Exporting Results\n",
    "- The Earth Engine results are converted into a **Pandas DataFrame**.\n",
    "- The final table is saved as `lake_slope_elevation.csv`.\n",
    "\n",
    "Each row in the output represents:\n",
    "- one lake\n",
    "- its **mean elevation**\n",
    "- its **mean slope**\n",
    "\n",
    "---\n",
    "\n",
    "#### Conceptual Significance\n",
    "This workflow measures **how flat or bowl-shaped each lake actually is**, using its **true spatial extent**. Flat, low-slope lakes are more likely to be **filled, encroached, or hydrologically compromised**, while steeper basins indicate **better-preserved lake morphology**.\n",
    "\n",
    "---\n",
    "\n",
    "#### One-line takeaway\n",
    "We are extracting **physically meaningful topographic indicators** (slope and elevation) **directly from exact lake boundaries**, enabling robust analysis of lake degradation and flood vulnerability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd489dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "from shapely import wkt # To parse the WKT geometry\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "df = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "df_boundary = pd.read_csv('data/lake_polygon_boundaries.csv')\n",
    "df_boundary = df_boundary.drop_duplicates(subset='name')\n",
    "df = df.merge(df_boundary, on = 'name', how = 'left')\n",
    "\n",
    "# 2. Convert Pandas DataFrame (with geometries) to EE FeatureCollection\n",
    "features = []\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['geometry']):\n",
    "        # Parse the WKT string\n",
    "        poly = wkt.loads(row['geometry'])\n",
    "        \n",
    "        if isinstance(poly, Polygon):\n",
    "            # Single Polygon: Create a list containing one ring\n",
    "            coords = [list(poly.exterior.coords)]\n",
    "            geom = ee.Geometry.Polygon(coords)\n",
    "            \n",
    "        elif isinstance(poly, MultiPolygon):\n",
    "            # MultiPolygon: Iterate through all constituent polygons\n",
    "            all_rings = []\n",
    "            for p in poly.geoms:\n",
    "                all_rings.append([list(p.exterior.coords)])\n",
    "            geom = ee.Geometry.MultiPolygon(all_rings)\n",
    "            \n",
    "        features.append(ee.Feature(geom, {'name': row['name']}))\n",
    "\n",
    "lake_polygons = ee.FeatureCollection(features)\n",
    "\n",
    "# 3. Topographic Analysis\n",
    "srtm = ee.Image(\"USGS/SRTMGL1_003\")\n",
    "elevation = srtm.select('elevation')\n",
    "slope = ee.Terrain.slope(elevation).rename('slope')\n",
    "topo_stack = ee.Image.cat([elevation, slope])\n",
    "\n",
    "# 4. Extract Stats (Mean values within the EXACT boundaries)\n",
    "stats = topo_stack.reduceRegions(\n",
    "    collection=lake_polygons,\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    scale=30 \n",
    ")\n",
    "\n",
    "# 5. Export to CSV\n",
    "df_results = geemap.ee_to_df(stats)\n",
    "df_results.to_csv('data/lake_slope_elevation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2038d40",
   "metadata": {},
   "source": [
    "## Extracting Hydrological Context for Bengalurus Lakes\n",
    "\n",
    "This section of the code prepares the **spatial boundary**, **lake locations**, and **hydrological flow layers** needed to understand how water moves across Bengaluru and interacts with its lakes.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Defining the Bengaluru Urban Boundary\n",
    "\n",
    "```python\n",
    "bengaluru_boundary = ee.FeatureCollection(\"FAO/GAUL/2015/level2\")\n",
    "    .filter(ee.Filter.eq('ADM2_NAME', 'Bangalore Urban'))\n",
    "```\n",
    "\n",
    "### 2. Loading Lake Locations as Point Features\n",
    "\n",
    "```python \n",
    "df = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "features = [\n",
    "    ee.Feature(\n",
    "        ee.Geometry.Point([row['lon'], row['lat']]),\n",
    "        {'name': row['name']}\n",
    "    ) \n",
    "    for i, row in df.iterrows()\n",
    "]\n",
    "lake_points = ee.FeatureCollection(features)\n",
    "```\n",
    "**What is happening**\n",
    "\n",
    "* Reads a CSV file containing lake centroids.\n",
    "\n",
    "* Converts each latitudelongitude pair into:\n",
    "\n",
    "    * An Earth Engine Point geometry\n",
    "\n",
    "    * With lake name as metadata\n",
    "\n",
    "* All points are combined into a FeatureCollection.\n",
    "\n",
    "**Why this matters**\n",
    "* Lake points act as anchors to sample hydrological properties.\n",
    "\n",
    "* Enables point-based queries such as:\n",
    "\n",
    "    * Upstream contributing area\n",
    "\n",
    "    * Flow direction at the lake location\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Loading MERIT Hydro Datasets\n",
    "`merit = ee.Image(\"MERIT/Hydro/v1_0_1\")`\n",
    "\n",
    "**What this dataset is**\n",
    "\n",
    "* **MERIT Hydro** is a globally corrected hydrological dataset.\n",
    "\n",
    "* Built on improved DEMs with:\n",
    "\n",
    "    * Reduced striping errors\n",
    "\n",
    "    * Corrected river networks\n",
    "\n",
    "* It is especially useful for urban flood and drainage analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Extracting Flow Accumulation\n",
    "```python\n",
    "flow_acc = merit.select('upa')\n",
    "flow_acc_viz = flow_acc.log10()\n",
    "```\n",
    "\n",
    "**Key concepts**\n",
    "\n",
    "* `upa` (Upstream Accumulation Area)\n",
    "\n",
    "* Represents the total upstream area draining into each pixel.\n",
    "\n",
    "* High values indicate major drains and valleys.\n",
    "\n",
    "**Why log-transform**\n",
    "\n",
    "* Raw flow accumulation values span several orders of magnitude.\n",
    "\n",
    "* Log transformation:\n",
    "\n",
    "    * Enhances visibility of small urban streams\n",
    "\n",
    "    * Prevents large rivers from dominating the visualization\n",
    "\n",
    "**Hydrological meaning**\n",
    "\n",
    "* Pixels with high values indicate where runoff naturally converges.\n",
    "\n",
    "* Lakes located on high upa pixels are structurally flood-prone.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Extracting Flow Direction\n",
    "`flow_dir = merit.select('dir')`\n",
    "\n",
    "**What dir represents**\n",
    "\n",
    "* Indicates the direction water flows out of each pixel.\n",
    "\n",
    "* Encoded using a D8 flow model (8 possible directions).\n",
    "\n",
    "**Flow direction explains**:\n",
    "\n",
    "* How water moves between lakes\n",
    "\n",
    "* Which lakes are upstream or downstream\n",
    "\n",
    "* Essential for understanding Bengalurus historic cascade lake system.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Preparing the Map for Visualisation\n",
    "```python\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(bengaluru_boundary, 11)\n",
    "```\n",
    "\n",
    "**What is happening**\n",
    "\n",
    "* Initializes an interactive map.\n",
    "\n",
    "* Centers the map over Bangalore Urban at a city-scale zoom level.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Flow Accumulation Visualisation Parameters\n",
    "```python\n",
    "acc_params = {\n",
    "    'min': 0, \n",
    "    'max': 5, \n",
    "    'palette': ['#000000', '#023858', '#0570b0', '#74a9cf', '#fff7fb']\n",
    "}\n",
    "```\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "* Dark colors  low or negligible drainage\n",
    "\n",
    "* Light colors  strong drainage pathways\n",
    "\n",
    "**Highlights**:\n",
    "\n",
    "* Natural valleys\n",
    "\n",
    "* Stormwater drains\n",
    "\n",
    "* Low-lying convergence zones\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Flow Direction Visualisation Parameters\n",
    "\n",
    "```python\n",
    "dir_params = {\n",
    "    'min': 1, \n",
    "    'max': 128, \n",
    "    'palette': ['red', 'orange', 'yellow', 'green', 'blue', 'cyan', 'magenta', 'black']\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "* Each color corresponds to a specific flow direction.\n",
    "\n",
    "* Together, they reveal the directional logic of runoff across the city.\n",
    "\n",
    "* Helps visually verify:\n",
    "\n",
    "    * Whether lakes align with natural flow paths\n",
    "\n",
    "    * Where drainage has been disrupted by urban development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get Boundary and Lakes\n",
    "bengaluru_boundary = ee.FeatureCollection(\"FAO/GAUL/2015/level2\") \\\n",
    "    .filter(ee.Filter.eq('ADM2_NAME', 'Bangalore Urban'))\n",
    "\n",
    "# Load your lake points\n",
    "df = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "features = [ee.Feature(ee.Geometry.Point([row['lon'], row['lat']]), {'name': row['name']}) for i, row in df.iterrows()]\n",
    "lake_points = ee.FeatureCollection(features)\n",
    "\n",
    "# 3. Load MERIT Hydro Datasets with CORRECT BAND NAMES\n",
    "merit = ee.Image(\"MERIT/Hydro/v1_0_1\")\n",
    "\n",
    "# 'upa' is the band for Upstream Accumulation Area\n",
    "flow_acc = merit.select('upa') \n",
    "# We log-transform it for better visualization of small streams\n",
    "flow_acc_viz = flow_acc.log10() \n",
    "\n",
    "# 'dir' is the band for Flow Direction\n",
    "flow_dir = merit.select('dir')\n",
    "\n",
    "# 4. Visualization on the Map\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(bengaluru_boundary, 11)\n",
    "\n",
    "# Palette for Flow Accumulation (Blue to White represents the drainage network)\n",
    "acc_params = {\n",
    "    'min': 0, \n",
    "    'max': 5, \n",
    "    'palette': ['#000000', '#023858', '#0570b0', '#74a9cf', '#fff7fb']\n",
    "}\n",
    "\n",
    "# Palette for Direction (Standard 8-direction colors)\n",
    "dir_params = {\n",
    "    'min': 1, \n",
    "    'max': 128, \n",
    "    'palette': ['red', 'orange', 'yellow', 'green', 'blue', 'cyan', 'magenta', 'black']\n",
    "}\n",
    "\n",
    "Map.addLayer(flow_dir.clip(bengaluru_boundary), dir_params, '1. Flow Direction (Compass)')\n",
    "Map.addLayer(flow_acc_viz.clip(bengaluru_boundary), acc_params, '2. Flow Accumulation (Drainage Network)')\n",
    "Map.addLayer(lake_points, {'color': 'red'}, '3. Lake Locations')\n",
    "\n",
    "Map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a25aa2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting flow stats for lakes...\n",
      "Success! Data saved to data/lake_flow_analysis.csv\n",
      "                          name  flow_accumulation_km2\n",
      "152  Yellamallappa Chetty Lake             267.274902\n",
      "114              Ramapura Kere             224.145035\n",
      "136             Bellandur Lake             114.820732\n",
      "154                  Saul Kere              23.875479\n",
      "85                    Hosakere              16.710138\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract and Save Data\n",
    "print(\"Extracting flow stats for lakes...\")\n",
    "# Combine bands into one image for sampling\n",
    "topo_image = ee.Image.cat([\n",
    "    flow_acc.rename('flow_accumulation_km2'),\n",
    "    flow_dir.rename('flow_direction_code')\n",
    "])\n",
    "\n",
    "stats = topo_image.reduceRegions(\n",
    "    collection=lake_points,\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    scale=90\n",
    ")\n",
    "\n",
    "try:\n",
    "    df_results = geemap.ee_to_df(stats)\n",
    "    if not os.path.exists('data'): os.makedirs('data')\n",
    "    df_results.to_csv('data/lake_flow_analysis.csv', index=False)\n",
    "    print(\"Success! Data saved to data/lake_flow_analysis.csv\")\n",
    "    print(df_results[['name', 'flow_accumulation_km2']].sort_values(by='flow_accumulation_km2', ascending=False).head())\n",
    "except Exception as e:\n",
    "    print(f\"Error saving CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6602ef8a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Lake-wise SAR (Synthetic Aperture Radar) Flood Frequency Extraction\n",
    "\n",
    "This script computes **observed flood frequency** around each Bengaluru lake using **Sentinel-1 SAR radar data** for the period **20202025**. The output is a lake-level dataset showing **how often flooding was actually detected**, based on satellite observations rather than model assumptions.\n",
    "\n",
    "---\n",
    "\n",
    "**Initialization and Data Loading**\n",
    "\n",
    "- Earth Engine is initialized so all geospatial processing runs on Googles servers.\n",
    "- A CSV containing lake names and coordinates is loaded into a Pandas DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "**Building a City-wide Flood Frequency Image (Done Once)**\n",
    "\n",
    "- All lake coordinates are combined into a single **MultiPoint geometry**, then buffered by **500 m** to define a city-wide region of interest.\n",
    "- Sentinel-1 SAR images are loaded and filtered:\n",
    "  - Spatially: only images covering the buffered lake region\n",
    "  - Temporally: 20202025\n",
    "  - Polarisation: **VH** (best for water detection in urban areas)\n",
    "  - Mode: **IW** (standard land observation mode)\n",
    "\n",
    "- Each SAR image is converted into a **binary water map** using a 20 dB threshold:\n",
    "  - Values below 20 dB  likely water/flooded\n",
    "  - Values above 20 dB  land or built-up\n",
    "- All binary water maps are stacked over time.\n",
    "- **Flood frequency (%)** is computed per pixel as:  \n",
    "  *(number of times water was detected  number of observations)  100*  \n",
    "  This produces a single raster (`sar_flood_freq_pct`) showing how often each pixel was inundated over five years.\n",
    "\n",
    "---\n",
    "\n",
    "**Lake-wise Extraction Loop**\n",
    "\n",
    "- The script loops through lakes **client-side (Python)** for progress monitoring.\n",
    "- For each lake:\n",
    "  - A **200 m buffer** around the lake centroid is created to capture spillover and nearby waterlogging.\n",
    "  - The **mean flood frequency** within this buffer is extracted from the precomputed SAR flood-frequency image using `reduceRegion`.\n",
    "  - This yields one number per lake:  \n",
    "    *On average, what percentage of satellite passes detected flooding here?*\n",
    "\n",
    "- Results are stored in a list with lake name and flood frequency.\n",
    "- Errors for individual lakes are caught so the loop continues uninterrupted.\n",
    "\n",
    "---\n",
    "\n",
    "**Saving the Output**\n",
    "\n",
    "- The collected results are converted to a DataFrame.\n",
    "- A CSV file is written containing:\n",
    "  - `name`  lake name  \n",
    "  - `sar_flood_freq_pct`  observed flood frequency (20202025)\n",
    "\n",
    "---\n",
    "\n",
    "**Conceptual Meaning**\n",
    "\n",
    "- This workflow provides an **empirical, observation-based measure of flooding**, not a simulated one.\n",
    "- It captures:\n",
    "  - chronic waterlogging\n",
    "  - repeated lake overflows\n",
    "  - drainage failures\n",
    "- The output is ideal for:\n",
    "  - validating flood models\n",
    "  - identifying flood hotspots\n",
    "  - serving as a **ground-truth target** for machine-learning flood-risk models\n",
    "\n",
    "---\n",
    "\n",
    "**One-line takeaway**\n",
    "\n",
    "This code converts five years of Sentinel-1 radar imagery into a lake-wise measure of how often flooding actually occurred around Bengalurus lakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d81297d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction for 162 lakes...\n",
      "[162/162] Processing: Vidyaranyapura Kere...ake...e...\n",
      "Extraction Complete! File saved.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 1. Initialize\n",
    "ee.Initialize(project='bengaluru-lakes-485612')\n",
    "\n",
    "# 2. Load Data\n",
    "df_lakes = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "\n",
    "# 3. Create the Base Frequency Image (Do this ONCE outside the loop)\n",
    "roi_all = ee.Geometry.MultiPoint(df_lakes[['lon', 'lat']].values.tolist()).buffer(500)\n",
    "s1_collection = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "    .filterBounds(roi_all) \\\n",
    "    .filterDate('2020-01-01', '2025-12-31') \\\n",
    "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH')) \\\n",
    "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "\n",
    "def identify_water(image):\n",
    "    return image.select('VH').lt(-20).rename('is_water').copyProperties(image, ['system:time_start'])\n",
    "\n",
    "water_ts = s1_collection.map(identify_water)\n",
    "flood_freq_img = water_ts.sum().divide(water_ts.count()).multiply(100).rename('sar_flood_freq_pct')\n",
    "\n",
    "# 4. Processing Loop with Prints\n",
    "results = []\n",
    "print(f\"Starting extraction for {len(df_lakes)} lakes...\")\n",
    "\n",
    "for index, row in df_lakes.iterrows():\n",
    "    lake_name = row['name']\n",
    "    print(f\"[{index+1}/{len(df_lakes)}] Processing: {lake_name}...\", end=\"\\r\")\n",
    "    \n",
    "    # Define local geometry\n",
    "    point = ee.Geometry.Point([row['lon'], row['lat']]).buffer(200)\n",
    "    \n",
    "    # Extract mean frequency for this specific lake\n",
    "    try:\n",
    "        # reduceRegion (singular) is faster for a single geometry\n",
    "        stat = flood_freq_img.reduceRegion(\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            geometry=point,\n",
    "            scale=10,\n",
    "            maxPixels=1e9\n",
    "        ).getInfo()\n",
    "        \n",
    "        results.append({\n",
    "            'name': lake_name,\n",
    "            'sar_flood_freq_pct': stat.get('sar_flood_freq_pct')\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {lake_name}: {e}\")\n",
    "\n",
    "# 5. Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('data/lake_sar_flood_frequency_2025.csv', index=False)\n",
    "print(\"\\nExtraction Complete! File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d23deb",
   "metadata": {},
   "source": [
    "## What this code does and why it is useful\n",
    "\n",
    "### Purpose  \n",
    "This code computes a **true flood-frequency metric around lakes** using **Sentinel-1 SAR data**, by distinguishing **episodic flooding** from **permanent lake water**. It produces a lake-wise percentage indicating how often areas around each lake experience flooding over a multi-year period (20202025).\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-step explanation  \n",
    "\n",
    "#### 1. Lake input and region of interest  \n",
    "- Reads a CSV containing lake names and their latitudelongitude coordinates.  \n",
    "- Combines all lake points into a **single buffered region of interest (ROI)**.  \n",
    "- This optimisation ensures Sentinel-1 data are loaded only once for the entire study area, improving efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Sentinel-1 SAR data selection  \n",
    "- Loads **Sentinel-1 GRD** imagery within the ROI and time window (20202025).  \n",
    "- Filters for:\n",
    "  - **IW mode** (standard for land applications)  \n",
    "  - **VH polarisation**, which is sensitive to open water  \n",
    "- Selects only the VH band to reduce data volume.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Water detection  \n",
    "- Converts each SAR image into a **binary water mask** using a backscatter threshold (VH < 20 dB).  \n",
    "- This exploits the physical property that smooth water surfaces return very low radar backscatter.  \n",
    "- Temporal metadata is preserved for time-series analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Dry-season baseline (normal lake extent)  \n",
    "- Restricts the water masks to **JanuaryMarch**, when flooding is minimal.  \n",
    "- Computes the mean water occurrence during this dry season.  \n",
    "- Pixels classified as water in 70% of dry-season observations are treated as **permanent lake water**.  \n",
    "- This baseline represents the *normal, non-flooded lake extent*.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Flood detection (core logic)  \n",
    "- For every SAR acquisition, identifies **flood pixels** as:\n",
    "  - Water present **outside** the dry-season baseline.  \n",
    "- This step removes permanent lake water and isolates **anomalous inundation**, which is the defining feature of flooding.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Flood frequency calculation  \n",
    "- Sums all flood detections across time.  \n",
    "- Divides by the number of valid observations.  \n",
    "- Converts the result into a **percentage flood frequency image**, indicating how often flooding occurred at each pixel.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Lake-wise flood-frequency extraction  \n",
    "- Iterates over each lake.  \n",
    "- Uses a local buffer around the lake centre (proxy for surrounding flood-prone area).  \n",
    "- Computes the **mean flood frequency (%)** within that buffer.  \n",
    "- Stores results lake by lake for further analysis or export.\n",
    "\n",
    "---\n",
    "\n",
    "### Utility of this approach  \n",
    "\n",
    "- **Conceptually correct**: Measures flooding as a deviation from normal conditions, not simple water presence.  \n",
    "- **SAR-based**: Works reliably during monsoon months and under cloud cover.  \n",
    "- **Comparative**: Enables ranking of lakes by chronic flood exposure.  \n",
    "- **Policy-relevant**: Useful for urban planning, drainage prioritisation, lake rejuvenation, and resilience studies.  \n",
    "- **Scalable**: Can be extended to polygons, rainfall conditioning, or adaptive thresholds.\n",
    "\n",
    "In short, this code transforms raw SAR imagery into a defensible, event-based **flood-frequency indicator** around urban lakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "276e950d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting flood-frequency extraction for 162 lakes...\n",
      "[162/162] Processing Vidyaranyapura Kerey LakeLake\n",
      "Flood-frequency extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. INITIALISE EARTH ENGINE\n",
    "# -------------------------------------------------------------------\n",
    "ee.Initialize(project='bengaluru-lakes-485612')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. LOAD LAKE POINT DATA\n",
    "# -------------------------------------------------------------------\n",
    "df_lakes = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "\n",
    "# Combined ROI for efficiency\n",
    "roi_all = ee.Geometry.MultiPoint(\n",
    "    df_lakes[['lon', 'lat']].values.tolist()\n",
    ").buffer(500)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. LOAD SENTINEL-1 SAR DATA\n",
    "# -------------------------------------------------------------------\n",
    "s1 = (\n",
    "    ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "    .filterBounds(roi_all)\n",
    "    .filterDate('2020-01-01', '2025-12-31')\n",
    "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "    .filter(ee.Filter.listContains(\n",
    "        'transmitterReceiverPolarisation', 'VH'\n",
    "    ))\n",
    "    .select('VH')\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. WATER DETECTION FUNCTION\n",
    "# -------------------------------------------------------------------\n",
    "def detect_water(image):\n",
    "    water = image.lt(-20)  # heuristic threshold\n",
    "    return water.rename('water').copyProperties(\n",
    "        image, ['system:time_start']\n",
    "    )\n",
    "\n",
    "water_series = s1.map(detect_water)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. DRY-SEASON BASELINE WATER MASK (NORMAL LAKE EXTENT)\n",
    "# -------------------------------------------------------------------\n",
    "dry_season = water_series.filter(\n",
    "    ee.Filter.calendarRange(1, 3, 'month')  # JanMar\n",
    ")\n",
    "\n",
    "# Permanent water = water in 70% of dry-season observations\n",
    "baseline_water = (\n",
    "    dry_season.mean()\n",
    "    .gt(0.7)\n",
    "    .rename('baseline_water')\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. FLOOD DETECTION (KEY STEP)\n",
    "# -------------------------------------------------------------------\n",
    "def detect_flood(image):\n",
    "    flood = image.And(baseline_water.Not())\n",
    "    return flood.rename('flood').copyProperties(\n",
    "        image, ['system:time_start']\n",
    "    )\n",
    "\n",
    "flood_series = water_series.map(detect_flood)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. FLOOD FREQUENCY IMAGE (% OF OBSERVATIONS)\n",
    "# -------------------------------------------------------------------\n",
    "flood_freq_img = (\n",
    "    flood_series.sum()\n",
    "    .divide(flood_series.count())\n",
    "    .multiply(100)\n",
    "    .rename('flood_freq_pct')\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8. LAKE-WISE EXTRACTION\n",
    "# -------------------------------------------------------------------\n",
    "results = []\n",
    "total = len(df_lakes)\n",
    "\n",
    "print(f\"Starting flood-frequency extraction for {total} lakes...\")\n",
    "\n",
    "for i, row in df_lakes.iterrows():\n",
    "    lake_name = row['name']\n",
    "    print(f\"[{i+1}/{total}] Processing {lake_name}\", end=\"\\r\")\n",
    "\n",
    "    # Local analysis buffer (replace with polygon if available)\n",
    "    lake_geom = ee.Geometry.Point(\n",
    "        [row['lon'], row['lat']]\n",
    "    ).buffer(200)\n",
    "\n",
    "    try:\n",
    "        stat = flood_freq_img.reduceRegion(\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            geometry=lake_geom,\n",
    "            scale=10,\n",
    "            maxPixels=1e9\n",
    "        ).getInfo()\n",
    "\n",
    "        results.append({\n",
    "            'name': lake_name,\n",
    "            'flood_freq_pct': stat.get('flood_freq_pct')\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {lake_name}: {e}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 9. SAVE OUTPUT\n",
    "# -------------------------------------------------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\n",
    "    'data/lake_true_flood_frequency_2025.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\nFlood-frequency extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab1a0f",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Measuring Rainfall Intensity and Timing\n",
    "* For urban flooding in Bengaluru, \"**Total Rainfall**\" is less important than \"**Intensity**\" (how much rain falls in a short window). \n",
    "* We use the **GPM (Global Precipitation Measurement) IMERG dataset**, which provides data every 30 minutes.\n",
    "    * **Metric 1 (Intensity): Max Daily Rainfall (mm/day)**.\n",
    "    * **Metric 2 (Timing)**: The month of the peak rainfall event (to correlate with your SAR flood observations).\n",
    "\n",
    "---\n",
    "\n",
    "## Measuring Imperviousness\n",
    "* \"Imperviousness\" refers to surfaces like concrete, asphalt, and rooftops that prevent water from soaking into the ground. \n",
    "* A high impervious percentage in the **200m buffer** around a lake leads to rapid runoff and higher flood risk.\n",
    "    * Dataset: **Dynamic World (10m) or ESA WorldCover**. Dynamic World is preferred because it's at **10m resolution (same as Sentinel-2).**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Initialize\n",
    "ee.Initialize(project='bengaluru-lakes-485612')\n",
    "\n",
    "# 2. Load Geometries\n",
    "df_lakes = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "features = [\n",
    "    ee.Feature(ee.Geometry.Point([row['lon'], row['lat']]).buffer(200), {'name': row['name']}) \n",
    "    for _, row in df_lakes.iterrows()\n",
    "]\n",
    "lake_fc = ee.FeatureCollection(features)\n",
    "\n",
    "def export_hydrology_year(year):\n",
    "    print(f\"Submitting Task for {year}...\")\n",
    "    start_date = ee.Date.fromYMD(year, 1, 1)\n",
    "    end_date = ee.Date.fromYMD(year, 12, 31)\n",
    "\n",
    "    # --- 1. RAINFALL: DAILY AGGREGATION ---\n",
    "    gpm = ee.ImageCollection(\"NASA/GPM_L3/IMERG_V07\") \\\n",
    "        .filterDate(start_date, end_date) \\\n",
    "        .select('precipitation')\n",
    "\n",
    "    days = ee.List.sequence(0, end_date.difference(start_date, 'day').subtract(1))\n",
    "    \n",
    "    def calc_daily(d):\n",
    "        date = start_date.advance(d, 'day')\n",
    "        return gpm.filterDate(date, date.advance(1, 'day')) \\\n",
    "                  .sum().multiply(0.5) \\\n",
    "                  .set('system:time_start', date.millis())\n",
    "    \n",
    "    daily_col = ee.ImageCollection.fromImages(days.map(calc_daily))\n",
    "    daily_list = daily_col.toList(366)\n",
    "\n",
    "    # --- 2. VECTORIZED ROLLING 3-DAY SUM (Faster) ---\n",
    "    # We sum Image(i) + Image(i-1) + Image(i-2)\n",
    "    indices = ee.List.sequence(2, daily_list.length().subtract(1))\n",
    "    \n",
    "    def sum_3days(i):\n",
    "        i = ee.Number(i)\n",
    "        img1 = ee.Image(daily_list.get(i))\n",
    "        img2 = ee.Image(daily_list.get(i.subtract(1)))\n",
    "        img3 = ee.Image(daily_list.get(i.subtract(2)))\n",
    "        return img1.add(img2).add(img3).set('system:time_start', img1.get('system:time_start'))\n",
    "\n",
    "    max_3day_img = ee.ImageCollection.fromImages(indices.map(sum_3days)).max().rename('max_3day_rain_mm')\n",
    "\n",
    "    # --- 3. PEAK INTENSITY & IMPERVIOUSNESS ---\n",
    "    peak_30min_img = gpm.max().multiply(0.5).rename('peak_30min_intensity_mm')\n",
    "    \n",
    "    dw = ee.ImageCollection(\"GOOGLE/DYNAMICWORLD/V1\") \\\n",
    "        .filterDate(start_date, end_date).select('label').mode()\n",
    "    impervious_img = dw.eq(6).rename('impervious_fraction')\n",
    "\n",
    "    # --- 4. BATCH EXTRACTION ---\n",
    "    combined = peak_30min_img.addBands([max_3day_img, impervious_img])\n",
    "    \n",
    "    stats = combined.reduceRegions(\n",
    "        collection=lake_fc,\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        scale=10,\n",
    "        tileScale=4 # Splits the job into smaller tiles to avoid memory errors\n",
    "    )\n",
    "\n",
    "    # --- 5. EXPORT TO DRIVE ---\n",
    "    task = ee.batch.Export.table.toDrive(\n",
    "        collection=stats,\n",
    "        description=f'Hydrology_Stats_{year}',\n",
    "        folder='EE_Exports', # Folder name in your Google Drive\n",
    "        fileNamePrefix=f'lake_stats_{year}',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    task.start()\n",
    "\n",
    "# Run for all years\n",
    "for yr in range(2020, 2026):\n",
    "    export_hydrology_year(yr)\n",
    "\n",
    "print(\"All tasks submitted! Check your Google Earth Engine 'Tasks' tab or your Google Drive 'EE_Exports' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8cdd1",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Recorded data cleaning for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load all datasets\n",
    "df_hydro = pd.read_csv('data/lake_stats_summary_2020_2025.csv')\n",
    "df_landuse = pd.read_csv('data/bengaluru_lakes_cleaned_gt_0.5ha.csv')\n",
    "df_flow = pd.read_csv('data/lake_flow_analysis.csv')\n",
    "df_flood = pd.read_csv('data/lake_sar_flood_frequency_2025.csv')\n",
    "df_encroach = pd.read_csv('data/bengaluru_lakes_mean.csv')\n",
    "\n",
    "# 2. Average the Yearly Data (Hydro & Land Use)\n",
    "# We drop 'year' and 'Unnamed: 0' before averaging\n",
    "hydro_mean = df_hydro.drop(columns=['year', 'Unnamed: 0'], errors='ignore').groupby('name').mean().reset_index()\n",
    "\n",
    "# For landuse, we keep lat/lon as they are constant, but average the areas\n",
    "landuse_mean = df_landuse.drop(columns=['year', 'Unnamed: 0'], errors='ignore').groupby('name').mean().reset_index()\n",
    "\n",
    "# 3. Merge into a single \"Representative\" DataFrame\n",
    "# Start with landuse_mean as it contains lat/lon\n",
    "ml_dataset = pd.merge(landuse_mean, hydro_mean, on='name', how='inner')\n",
    "\n",
    "# Add static flow data\n",
    "ml_dataset = pd.merge(ml_dataset, df_flow, on='name', how='left')\n",
    "\n",
    "# Add pre-calculated encroachment data\n",
    "ml_dataset = pd.merge(ml_dataset, df_encroach[['name', 'encroachment_pct']], on='name', how='left')\n",
    "\n",
    "# Add the TARGET variable (Flood Frequency)\n",
    "ml_dataset = pd.merge(ml_dataset, df_flood, on='name', how='left')\n",
    "\n",
    "# 4. Final Cleanup\n",
    "ml_dataset.fillna(0, inplace=True)\n",
    "\n",
    "# 5. Save for ML\n",
    "ml_dataset.to_csv('data/lake_flood_ml_ready.csv', index=False)\n",
    "\n",
    "print(f\"ML Dataset Created: {ml_dataset.shape[0]} lakes and {ml_dataset.shape[1]} features.\")\n",
    "print(\"Sample of predictors:\", ml_dataset[['name', 'impervious_fraction', 'flow_accumulation_km2', 'sar_flood_freq_pct']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf1baf",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### MLBased Flood Risk Classification\n",
    "\n",
    "This script builds and evaluates a **lake-level flood risk classification model** for Bengaluru using **observed SAR flood frequency** as the outcome and a set of **physically meaningful flood drivers** as predictors. The goal is to classify lakes into **Low Risk** and **High Risk** flood categories in a way that is interpretable and actionable.\n",
    "\n",
    "---\n",
    "\n",
    "**Data Loading**\n",
    "\n",
    "- A pre-processed, lake-level dataset (`lake_flood_ml_ready.csv`) is loaded.\n",
    "- Each row represents one lake, with rainfall, land-cover, drainage, and observed flood-frequency metrics already aggregated spatially.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Selection: The Three Pillars Framework**\n",
    "\n",
    "- **Hydrological Drivers (Trigger):**\n",
    "  - `max_3day_rain_mm`  cumulative wetness / system saturation  \n",
    "  - `peak_30min_intensity_mm`  short-duration storm intensity\n",
    "\n",
    "- **Land-Cover Vulnerability (Resistance):**\n",
    "  - `impervious_fraction`  runoff efficiency  \n",
    "  - `in_build_ha`  built-up pressure near lakes  \n",
    "  - `encroachment_pct`  loss of natural buffer and storage\n",
    "\n",
    "- **Landscape Topology (Gravity):**\n",
    "  - `flow_accumulation_km2`  upstream drainage pressure  \n",
    "  - `potential_ha`  lake basin scale\n",
    "\n",
    "These features reflect **physical flood processes**, not just statistical convenience.\n",
    "\n",
    "---\n",
    "\n",
    "**Target Variable Construction**\n",
    "\n",
    "- The continuous SAR-derived flood frequency (`sar_flood_freq_pct`) is converted into a binary risk label.\n",
    "- Lakes with flood frequency **greater than 25%** are labelled as **High Risk** (`1`); others as **Low Risk** (`0`).\n",
    "- This threshold produces a policy-friendly flood-risk classification while preserving an observational basis.\n",
    "\n",
    "---\n",
    "\n",
    "**Data Cleaning and TrainTest Split**\n",
    "\n",
    "- Rows with missing feature or label values are removed.\n",
    "- The dataset is split into:\n",
    "  - **80% training data**\n",
    "  - **20% testing data**\n",
    "- A fixed random seed ensures reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Model Training**\n",
    "\n",
    "- A **Random Forest Classifier** with 100 decision trees is trained.\n",
    "- Random Forests are well suited here because:\n",
    "  - flood drivers interact non-linearly\n",
    "  - features operate at different scales\n",
    "  - the model remains interpretable via feature importance\n",
    "\n",
    "---\n",
    "\n",
    "**Model Evaluation**\n",
    "\n",
    "- Predictions are generated for the test set.\n",
    "- Performance is assessed using:\n",
    "  - **Accuracy**  overall correctness\n",
    "  - **Classification report**  precision, recall, and F1-score for Low and High Risk classes\n",
    "- This evaluates how well physical drivers explain observed flooding.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Importance Analysis**\n",
    "\n",
    "- The contribution of each feature to the models decisions is extracted.\n",
    "- Features are grouped by category (Rain, Buildings, Topology) to assess:\n",
    "  - which physical processes dominate flood risk\n",
    "- A bar plot visualizes relative importance for intuitive interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "**Saving Final Predictions**\n",
    "\n",
    "- Model predictions are mapped back to lake names.\n",
    "- The output CSV contains:\n",
    "  - lake name\n",
    "  - observed flood frequency\n",
    "  - true risk label\n",
    "  - predicted risk class\n",
    "- This enables direct comparison between observed and modelled flood risk.\n",
    "\n",
    "---\n",
    "\n",
    "**Conceptual Meaning**\n",
    "\n",
    "- This workflow translates **observed flooding patterns** into a **predictive, interpretable risk classification**.\n",
    "- It does not simulate floods; instead, it learns which combinations of rainfall, urbanisation, and drainage characteristics are associated with repeated inundation.\n",
    "- The results are suitable for:\n",
    "  - prioritising flood-prone lakes\n",
    "  - policy and planning discussions\n",
    "  - downstream regression or risk-index development\n",
    "\n",
    "---\n",
    "\n",
    "**One-line takeaway**\n",
    "\n",
    "This code uses a Random Forest classifier to learn how rainfall, urban encroachment, and drainage topology jointly determine whether Bengalurus lakes are repeatedly flood-prone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc72783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. LOAD AND MERGE\n",
    "df_ml = pd.read_csv('data/lake_flood_ml_ready.csv')\n",
    "df_topo = pd.read_csv('data/lake_slope_elevation.csv')\n",
    "df = df_ml.merge(df_topo[['name', 'elevation', 'slope']], on='name', how='left')\n",
    "\n",
    "# 2. FEATURE ENGINEERING\n",
    "# WIBI Proxy: Detects hard debris in dry lake beds\n",
    "df['wibi_proxy'] = df['encroachment_pct'] * (1 - (df['static_water_ha'] / df['potential_ha']))\n",
    "# Urban Stress: Runoff pressure (Imperviousness x Catchment Flow)\n",
    "df['urban_stress'] = df['impervious_fraction'] * df['flow_accumulation_km2']\n",
    "# CSR: Catchment-to-Storage Ratio\n",
    "df['csr_ratio'] = df['flow_accumulation_km2'] / (df['potential_ha'] + 0.01)\n",
    "\n",
    "# 3. DEFINE FEATURE PILLARS\n",
    "rain_feats = ['max_3day_rain_mm', 'peak_30min_intensity_mm']\n",
    "modification_feats = ['impervious_fraction', 'wibi_proxy', 'urban_stress']\n",
    "topology_feats = ['potential_ha', 'flow_accumulation_km2', 'csr_ratio', 'elevation', 'slope']\n",
    "\n",
    "X_cols = rain_feats + modification_feats + topology_feats\n",
    "df['risk_label'] = (df['sar_flood_freq_pct'] > 25).astype(int)\n",
    "df_ml_final = df.dropna(subset=X_cols + ['risk_label'])\n",
    "\n",
    "# 4. TRAIN MODEL\n",
    "X = df_ml_final[X_cols]\n",
    "y = df_ml_final['risk_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. EXPORT PREDICTIONS\n",
    "test_results = df_ml_final.loc[X_test.index, ['name', 'sar_flood_freq_pct', 'risk_label']].copy()\n",
    "test_results['predicted_risk'] = model.predict(X_test)\n",
    "test_results.to_csv('data/final_flood_risk_with_topo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f740d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "df_ml = pd.read_csv('data/lake_flood_ml_ready.csv')\n",
    "df_topo = pd.read_csv('data/lake_slope_elevation.csv')\n",
    "df = df_ml.merge(df_topo[['name', 'elevation', 'slope']], on='name', how='left')\n",
    "\n",
    "# Feature Engineering\n",
    "df['wibi_proxy'] = df['encroachment_pct'] * (1 - (df['static_water_ha'] / df['potential_ha']))\n",
    "df['urban_stress'] = df['impervious_fraction'] * df['flow_accumulation_km2']\n",
    "df['csr_ratio'] = df['flow_accumulation_km2'] / (df['potential_ha'] + 0.01)\n",
    "\n",
    "# Categorization for Analysis\n",
    "rain_feats = ['max_3day_rain_mm', 'peak_30min_intensity_mm']\n",
    "modification_feats = ['impervious_fraction', 'wibi_proxy', 'urban_stress']\n",
    "topology_feats = ['potential_ha', 'flow_accumulation_km2', 'csr_ratio', 'elevation', 'slope']\n",
    "\n",
    "X_cols = rain_feats + modification_feats + topology_feats\n",
    "category_map = {\n",
    "    'max_3day_rain_mm': 'Rainfall (Trigger)',\n",
    "    'peak_30min_intensity_mm': 'Rainfall (Trigger)',\n",
    "    'impervious_fraction': 'Urban Modification',\n",
    "    'wibi_proxy': 'Infilling Proxy (WIBI)',\n",
    "    'urban_stress': 'Urban Modification',\n",
    "    'potential_ha': 'Topology (Gravity)',\n",
    "    'flow_accumulation_km2': 'Topology (Gravity)',\n",
    "    'csr_ratio': 'Topology (Gravity)',\n",
    "    'elevation': 'Topology (Gravity)',\n",
    "    'slope': 'Topology (Gravity)'\n",
    "}\n",
    "\n",
    "# 2. MODEL TRAINING\n",
    "df['risk_label'] = (df['sar_flood_freq_pct'] > 25).astype(int)\n",
    "df_final = df.dropna(subset=X_cols + ['risk_label'])\n",
    "X = df_final[X_cols]; y = df_final['risk_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. VISUALIZATION: Feature Importance\n",
    "feat_imp = pd.DataFrame({'Feature': X_cols, 'Importance': model.feature_importances_})\n",
    "feat_imp['Category'] = feat_imp['Feature'].map(category_map)\n",
    "feat_imp = feat_imp.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feat_imp, x='Importance', y='Feature', hue='Category', dodge=False)\n",
    "plt.title('Drivers of Flood Risk: Feature Contribution')\n",
    "plt.savefig('flood_feature_importance.png')\n",
    "\n",
    "# 4. VISUALIZATION: Correlation Heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = df_final[X_cols + ['sar_flood_freq_pct']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation: Topography, Urbanization & Flooding')\n",
    "plt.savefig('flood_correlation_heatmap.png')\n",
    "\n",
    "# 5. VISUALIZATION: Topographic Impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.scatterplot(data=df_final, x='elevation', y='sar_flood_freq_pct', hue='risk_label', ax=ax1)\n",
    "ax1.set_title('Elevation vs Flood Frequency')\n",
    "sns.scatterplot(data=df_final, x='slope', y='sar_flood_freq_pct', hue='risk_label', ax=ax2)\n",
    "ax2.set_title('Slope vs Flood Frequency')\n",
    "plt.savefig('topography_impact.png')\n",
    "\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
